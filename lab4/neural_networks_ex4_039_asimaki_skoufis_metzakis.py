# -*- coding: utf-8 -*-
"""Neural_Networks_Ex4_039_Asimaki_Skoufis_Metzakis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MskRwB48OAxWS38YGa5kUVPj9jc-HwNq

# Παίζοντας Atari με Βαθιά Ενισχυτική Μάθηση

Ομάδα: 39  
Ασημάκη Γεωργία-Γρηγορία: 03116197, a.tzotzo@gmail.com  
Μετζάκης Ιωάννης: 03116202, johnmetzakis@gmail.com   
Σκούφης Πέτρος: 03116141, pskoufis13@gmail.com

## Ενεργοποίηση του Tensorboard και εγκατάσταση απαραίτητων βιβλιοθηκών
"""

!apt install apt-utils psmisc -y
!killall -9 ngrok
!rm -rf *ngrok*
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip
!mkdir -p ngrok-dir
!mv ngrok ngrok-dir/
# Στην επόμενη γραμμή βάλτε το δικό σας authentication token από το ngrok
!ngrok-dir/ngrok authtoken 1oeYOqvBjrDmcGYz2wP7QMrAXjn_37t1QsHMt1zNnrb5a3rmb

LOG_DIR = 'tb_log/'
!mkdir -p LOG_DIR

get_ipython().system_raw(
    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'
    .format(LOG_DIR)
)
get_ipython().system_raw('ngrok-dir/ngrok http 6006 &')
! curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"
print("Wait 10-15 seconds and then click the URL above to open TensorBoard")

!pip install --upgrade pip
!pip install --upgrade stable_baselines3[extra]
# we need a specific version of gym because of this issue: https://github.com/DLR-RM/stable-baselines3/issues/294
!pip install gym==0.17.3

"""## Ορισμός και λίγα λόγια για το παιχνίδι

Στην ομάδα μας αντιστοιχεί το παιχνίδι **"Private Eye"**.  
        
Λίγα λόγια για το παιχνίδι:

- Στο Private Eye ο παίχτης έχει τον ρόλο του *Pierre Touche*, ενός ιδιωτικού ερευνητή με στόχο τη σύλληψη του διαβόητου εγκληματία *Henri Le Fiend*. Αυτό το επιτυγχάνει συλλέγοντας στοιχεία και αντικείμενα που σχετίζονται με το εκάστοτε έγκλημα.  

- Συγκεκριμένα, το παιχνίδι αποτελείται από 4 ξεχωριστές υποθέσεις, όπου η καθεμία αποτελεί μία διαφορετική εκδοχή του παιχνιδιού. Συνεπώς, για κάθε υπόθεση ισχύουν τα εξής:
 - Για να "κλέισει" μία υπόθεση, ο παίχτης πρέπει να αναζητήσει και να μεταφέρει ένα στοιχείο στο κατάλληλο μέρος για έλεγχο, και να επιστρέψει το κλεμμένο αντικείμενο στον νόμιμο ιδιοκτήτη του. Αφού ολοκληρωθούν οι αποστολές αυτές, πρέπει να εντοπίσει και να συλλάβει τον εγκληματία.
  - Ωστόσο, υπάρχουν εμπόδια, τα οποία ο παίχτης πρέπει να πηδήξει για να αποφύγει, καθώς και άλλοι κακοποιοί, οι οποίοι μπορεί να επιτεθούν στον παίχτη. Σε περίπτωση που ο παίχτης δεχτεί επίθεση ενώ μεταφέρει είτε το στοιχείο, είτε το κλεμμένο αντικείμενο, τότε αυτό χάνεται και πρέπει να εντοπιστεί εκ νέου.
 - Σημαντικότερο εμπόδιο όμως είναι ο χρόνος, ο οποίος κυλάει εις βάρος του παίχτη.  

- **Απόκτηση/Απώλεια πόντων:**
 - Ο παίχτης ξεκινάει με 1000 πόντους.
 - Πόντοι χάνονται όταν χτυπήσει ένα εμπόδιο ή δεχτεί επίθεση από κάποιον κακοποιό.
 - Αντιθέτως, πόντοι δίνονται όταν εντοπίσει ένα στοιχείο ή ένα αντικείμενο, όταν το επιστρέψει, και όταν ένας κακοποιός συλληφθεί.

## Random Agent

Για τον Random Agent, δοκιμάσαμε να τρέξουμε τον κώδικα που έδωσε ο κ. Σιόλας ως απάντηση σε μία απορία στη σελίδα του μαθήματος, ωστόσο συναντούσαμε πρόβλημα κατά την εισαγωγή ενός ακέραιου αριθμού "*actions*" στην "*env.step(action)*".  

Στη συνέχεια, αναζητήσαμε τη βιβλιογραφία της βιβλιοθήκης **Gym**, καταλήγοντας στον παρακάτω κώδικα, όπου στην ουσία προσθέσαμε την εντολή env.render(). Παρόλα αυτά, είχαμε και πάλι πρόβλημα, το οποίο μετά από σχετική αναζήτηση διαπιστώσαμε πως οφείλεται στην ασυμφωνία των εδκόσεων των βιβλιοθηκών.
"""

from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack

atari_env_name='PrivateEyeDeterministic-v4'

env = make_atari_env(atari_env_name, n_envs=1, seed=0) 
env = VecFrameStack(env, n_stack=4)

obs = env.reset()
for _ in range(10):
    env.render()
    action = env.action_space.sample()
    obs, reward, done, info = env.step(action) # take a random action
env.close()

"""Εν τέλει, δεν καταφέραμε να λύσουμε το συγκεκριμένο πρόβλημα, οπότε προχωρήσαμε στο να ελέγξουμε έναν τυχαίο πράκτορα σε ένα περιβάλλον, κάνοντας ακριβώς την ίδια διαδικασία που θα παρουσιάσουμε στη συνέχεια, χωρίς όμως το βήμα όπου εκπαιδεύουμε το μοντέλο "*model.learn()*".  

Έτσι, τα αποτελέσματα που πήραμε στο περιβάλλον ελέγχου (test) είναι τα εξής:  

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): -1000.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-0.0  

Όπως αναμέναμε, η μέση ανταμοιβή είναι εξαιρετικά χαμηλή, αφού το μοντέλο δεν έχει εκπαιδευτεί καθόλου.

## Εξέταση RL αλγορίθμων

Για τις ανάγκες της εργασίας χρησιμοποιήσαμε τους αλγορίθμους ενισχυτικής μάθησης της SB3. Aπαραίτητη προϋπόθεση για να μπορούν να δοκιμαστούν στα περιβάλλοντα Atari είναι το να έχουν διακριτό χώρο ενεργειών.  

Συνεπώς, εξετάσαμε τους εξής αλγορίθμους:

- DQN
- A2C
- PPO  
- QR-DQN (από το αποθετήριο SB3 contrib)  

Ωστόσο, λόγω του ότι δουλεύαμε σε 3 διαφορετικά notebooks ταυτόχρονα, και στην ουσία επαναλαμβάναμε τα ίδια κελιά με διαφορετικές παραμέτρους, θα αρκεστούμε στο να σας δείξουμε τα συγκεντρωτικά αποτελέσματα, παραθέτοντας παρακάτω τη διαδικασία που ακολουθήσαμε για κάθε configuration.

### Διαδικασία εκπαίδευσης ενός agent

Ορισμός ονόματος σύμφωνα με τον επεξηγηματικό πίνακα της εκφώνησης
"""

atari_env_name='PrivateEyeDeterministic-v4'

"""Δημιουργία περιβάλλοντος"""

from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack

# Με τις συναρτήσεις που ακολουθούν κάνουμε την ίδια προεπεξεργασία με την Deepmind

# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs
env = make_atari_env(atari_env_name, n_envs=4, seed=0) 
# Frame-stacking with 4 frames
# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)
env = VecFrameStack(env, n_stack=4)
# Test environment must be unique
test_env = make_atari_env(atari_env_name, n_envs=1)
# Frame-stacking with 4 frames
test_env = VecFrameStack(test_env, n_stack=4)

"""Εκπαίδευση"""

import datetime # For filenames while logging

from stable_baselines3 import A2C

model_name='a2c-MlpPolicy-deterministic'
time_stamp=datetime.datetime.now().strftime("-%Y%m%d-%H%M%S")
model_log= LOG_DIR + model_name + time_stamp

a2c_model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=model_log)

time_stamp_start =datetime.datetime.now().strftime("-%Y%m%d-%H%M%S")
max_steps=1000000   #μεγάλη τιμή = λιγότερες επαναλήψεις της ίδιας διαδικασίας, αλλά
                    #και ρίσκο να χαθεί μεγάλος όγκος δεδομένων σε περίπτωση σφάλματος
a2c_model.learn(total_timesteps=max_steps)
time_stamp=datetime.datetime.now().strftime("-%Y%m%d-%H%M%S")

"""Εκτίμηση απόδοσης"""

from stable_baselines3.common.evaluation import evaluate_policy

mean_reward, std_reward = evaluate_policy(a2c_model, test_env, n_eval_episodes=10, deterministic = True)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

"""Σώσιμο εκπαιδευμένου μοντέλου"""

a2c_model.save("a2c_private_eye_mlp_det1M.zip")

"""#### Μακροχρόνια εκπαίδευση

Φορτώνουμε το εκπαιδευμένο μοντέλο από το Dropbox, δημιουργούμε εκ νέου το περιβάλλον (ίδιο με αυτό που έγινε η αρχική εκπαίδευση του μοντέλου) και συνεχίζουμε σύμφωνα με την προηγούμενη διαδικασία.
"""

!wget --no-check-certificate 'https://www.dropbox.com/s/n6eqaw86kcfgosj/a2c_private_eye_cnn3M.zip?dl=1' -O a2c_private_eye_det3M.zip
a2c_model = A2C.load("a2c_private_eye_det3M", verbose=1)

#αυτό είναι με cnn αντί για mlp, αλλά είναι απλά για επίδειξη στη συγκεκριμένη περίπτωση

atari_env_name='PrivateEyeDeterministic-v4'
# Here we are also multi-worker training (n_envs=4 => 4 environments), The model must support Multi Processing. To DQN δεν επιτρέπει multi envs
env = make_atari_env(atari_env_name, n_envs=4, seed=0)   #isws thelei , seed=0 kai sto test to idio
# Frame-stacking with 4 frames
# με ένα frame έχουμε τη θέση, με δύο την ταχύτητα, με τρία την επιτάχυνση και με τέσσερα τον ρυθμό μεταβολής της επιτάχυνσης (jerk)
env = VecFrameStack(env, n_stack=4)
# Test environment must be unique
test_env = make_atari_env(atari_env_name, n_envs=1)
# Frame-stacking with 4 frames
test_env = VecFrameStack(test_env, n_stack=4)

a2c_model.set_env(env)

mean_reward, std_reward = evaluate_policy(a2c_model, test_env, n_eval_episodes=10, deterministic = True)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

"""Αυτή τη διαδικασία ακολουθήσαμε λοιπόν για κάθε configuration. Ακολουθούν τα συγκεντρωτικά αποτελέσματα για τα configurations κάθε μοντέλου που εξετάσαμε.

### **DQN**

#### Deterministic-v4 με Mlp-policy

- Έπειτα από 2.5 εκατομμύρια βήματα, πήραμε τα εξής αποτελέσματα (όπου η σειρά εκτέλεσης ανά 500000 βήματα είναι πορτοκαλί-μπλε-κόκκινο-γαλάζιο-ροζ, με το ροζ να αντιστοιχεί στην τελευταία εκτέλεση).


<img src="https://www.dropbox.com/s/62q2xv0wjbwcntz/dqn-mlp-det.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 80.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-40.0

Στη συνέχεια εξετάσαμε τον ίδιο αλγόριθμο, με το ίδιο configuration, αλλάζοντας όμως το policy από Mlp σε Cnn.

#### Deterministic-v4 με Cnn-policy

- Έπειτα από 2 εκατομμύρια βήματα, πήραμε τα εξής αποτελέσματα (όπου η σειρά εκτέλεσης ανά 500000 βήματα είναι πορτοκαλί-μπλε-κόκκινο-γαλάζιο, με το γαλάζιο να αντιστοιχεί στην τελευταία εκτέλεση).


<img src="https://www.dropbox.com/s/b0w877yoex7wlfz/dqn-cnn-det-v4.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 30.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-45.8

Για τις επόμενες δοκιμές, κρατήσαμε την τακτική Mlp-policy.

#### "Απλο"-v4 με Cnn-policy

- Έπειτα από 2 εκατομμύρια βήματα, πήραμε τα εξής αποτελέσματα (όπου η σειρά εκτέλεσης ανά 500000 βήματα είναι πορτοκαλί-μπλε-κόκκινο-γαλάζιο, με το γαλάζιο να αντιστοιχεί στην τελευταία εκτέλεση).


<img src="https://www.dropbox.com/s/7fkdz0wgvobh9d0/dqn-cnn-simple-v4.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 50.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-50.0

#### NoFrameskip-v4 με Cnn-policy

Στο συγκεκριμένο, είχαμε πρόβλημα κατά τον έλεγχο στο περιβάλλον test, καθώς έβγαζε μηδενική ανταμοιβή. Συνεπώς, θα δείξουμε μόνο την πορεία της εκπαίδευσης από το Tensorboard.  


<img src="https://www.dropbox.com/s/pz5mskpsqoa2ns0/dqn-cnn-noframe-v4.png?dl=1" alt="2" border="0"></a>

Στη συνέχεια δοκιμάσαμε το ντετερμινιστικό και το απλό περιβάλλον με στοχαστικότητα.

#### Deterministic-v0 με Cnn-policy

- Έπειτα από 2 εκατομμύρια βήματα, πήραμε τα εξής αποτελέσματα (όπου με πορτοκαλί βλέπουμε την πορεία της ανταμοιβής για τα πρώτα 1000000 βήματα, ενώ με μπλε τα τελικά).


<img src="https://www.dropbox.com/s/x8p39c9xhxqzbuf/dqn-cnn-det-v0.png?dl=1" alt="2" border="0"></a>  

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 86.3**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-16.9

#### "Απλό"-v0 με Cnn-policy

Όπως και σε προηγούμενη περίτπωση, είχαμε πρόβλημα κατά τον έλεγχο στο περιβάλλον test, καθώς έβγαζε μηδενική ανταμοιβή. Άρα και πάλι, θα δείξουμε μόνο την πορεία της εκπαίδευσης από το Tensorboard.  


<img src="https://www.dropbox.com/s/l84opa4kptl2yer/dqn-cnn-simple-v0.png?dl=1" alt="2" border="0"></a>

#### Γενικά συμπεράσματα

Παρατηρούμε πως το ντετερμινιστικό περιβάλλον μας δίνει τα καλύτερα αποτελέσματα.

Παρόλα αυτά, παρατηρούμε σημαντικές αυξομειώσεις σε κάθε περίπτωση. Το γεγονός αυτό οφείλεται στη δομή του παιχνιδιού, και πιο συγκεκριμένα στον τρόπο βαθμολόγησης, αφού μία κίνηση μπορεί να προσθαφαιρέσει αρκετούς πόντους.

### **A2C**

#### Deterministic-v4 με Mlp-policy

- Έπειτα από 1.5 εκατομμύριο βήματα, πήραμε τα εξής αποτελέσματα (όπου με πορτοκαλί βλέπουμε την πορεία της ανταμοιβής για τα πρώτα 500000 βήματα, με γαλάζιο τα ενδιάμεσα, και με κόκκινο τα τελικά).


<img src="https://www.dropbox.com/s/kr7si9h3ya9j4po/a2c-mlp-det.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 20.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-40.8

Στη συνέχεια εξετάσαμε τον ίδιο αλγόριθμο, με το ίδιο configuration, αλλάζοντας όμως το policy από Mlp σε Cnn.

#### Deterministic-v4 με Cnn-policy

- Έπειτα από 2 εκατομμύρια βήματα, πήραμε τα εξής αποτελέσματα (όπου με πορτοκαλί βλέπουμε την πορεία της ανταμοιβής για τα πρώτα 1000000 βήματα, ενώ με μπλε τα τελικά).


<img src="https://www.dropbox.com/s/m7zzhwnphterg06/a2c-cnn-det.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 70.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-45.5

Για τις επόμενες δοκιμές, κρατήσαμε την τακτική Mlp-policy.

#### "Απλο"-v4 με Cnn-policy

- Έπειτα από 2 εκατομμύρια βήματα, πήραμε τα εξής αποτελέσματα (όπου με πορτοκαλί βλέπουμε την πορεία της ανταμοιβής για τα πρώτα 1000000 βήματα, ενώ με μπλε τα τελικά).


<img src="https://www.dropbox.com/s/g9k6q8bqstlv08p/a2c-cnn-simple.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 40.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-59.0

#### Γενικά συμπεράσματα

Για NoFrameskip είχαμε ακόμα χειρότερα αποτελέσματα, με αρνητική μέση ανταμοιβή, και για αυτό το αφήσαμε.

Για όλους τους συνδυασμούς είχαμε αρκετά χαμηλή ανταμοιβή, ενώ παρατηρούσαμε πολύ έντονες και απότομες αυξομειώσεις, γεγονός που οφείλεται στον τρόπο βαθμολόγησης του παιχνιδιού, αφού μία κίνηση μπορεί να προσθαφαιρέσει αρκετούς πόντους.

Συνεπώς, με τόσο χαμηλή τιμή μέσης ανταμοιβής, δεν δοκιμάσαμε στοχαστικότητα, ολοκληρώνοντας έτσι την δοκιμή του συγκεκριμένου μοντέλου.

### **PPO**

#### Deterministic-v4 με Mlp-policy

- Έπειτα από 1 εκατομμύριο βήματα, πήραμε τα εξής αποτελέσματα (όπου με μαύρο βλέπουμε την πορεία της ανταμοιβής για τα πρώτα 500000 βήματα, ενώ με γαλάζιο τα τελικά - το ροζ αφορά το cnn-policy και έμεινε εκ παραδρομής στο συγκεκριμένο screenshot).


<img src="https://www.dropbox.com/s/c4kmunbb2r98q8h/ppo-mlp-det-v4.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 100.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-0.0

#### Determinisitic-v4 με Cnn-policy

- Έπειτα από μισό εκατομμύριο βήματα, πήραμε τα εξής αποτελέσματα (με ροζ, τα άλλα 2 είναι για το mlp-policy).


<img src="https://www.dropbox.com/s/c4kmunbb2r98q8h/ppo-mlp-det-v4.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 100.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-0.0

#### "Απλό"-v4 με Mlp-policy

- Έπειτα από 1 εκατομμύριο βήματα, πήραμε τα εξής αποτελέσματα (όπου με πορτοκαλί βλέπουμε την πορεία της ανταμοιβής για τα πρώτα 500000 βήματα, ενώ με μπλε τα τελικά).


<img src="https://www.dropbox.com/s/46x4qf44iygaweg/ppo-mlp-simple-v4.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 100.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-0.0

#### "Απλό"-v4 με Cnn-policy

- Έπειτα από 1.5 εκατομμύριο βήματα, πήραμε τα εξής αποτελέσματα (όπου με πορτοκαλί βλέπουμε την πορεία της ανταμοιβής για τα πρώτα 500000 βήματα, ενώ με κόκκινο τα τελικά).


<img src="https://www.dropbox.com/s/u6wvxvut45ffdgq/ppo-cnn-simple-v4.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 100.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-0.0

Για NoFrameskip είχαμε άκρως αντίθετα αποτελέσματα, με την ανταμοιβή να συγκεντρώνεται στο 0, οπότε το αφήσαμε.

#### Γενικά συμπεράσματα

Ο αλγόριθμος PPO, ο οποίος θα αναλυθεί και περισσότερο παρακάτω πετυχαίνει τα καλύτερα αποτελέσματα για το συγκεκριμένο παιχνίδι από αυτούς που δοκιμάσαμε, αν συνυπολογίσουμε τόσο το score όσο και τη διακύμανση. Παρ'όλα αυτά παρατηρούμε πως "κολλάει" στο score `100` το οποίο επιστρέφει και κατά το testing με **μηδενική** μάλιστα διακύμανση. 

Αφού εξετάσαμε το βίντεο παρατηρήσαμε πως ο πράκτοράς μας έχει βρεί έναν ιδιόρυθμο τρόπο να παίζει κάνοντας συνεχώς άλματα και αλλάζοντας συνεχώς μεταξύ των παραθύρων με σκοπό να μειώσει στο ελάχιστο την πιθανότητα να χάσει βαθμούς και να προτιμά να μένει σε αυτή τη σίγουρη κατάσταση.

Εξετάζοντας επίσης τις χρονοσειρές της εκπαίδευσης στο Tensorboard παρατηρήσαμε πως ακόμα και όταν η απόδοσή του μεταβαλλόταν είτε ανοδικά είτε καθοδικά,γρήγορα επέστρεφε στο σημείο ισορροπίας τους που είναι το `100`.

### **QR-DQN**

#### Deterministic-v4 με Mlp-policy

- Έπειτα από μισό εκατομμύριο βήματα, πήραμε τα εξής αποτελέσματα (παραθέτουμε και τα αντίστοιχα αποτελέσματα του ppo, με πορτοκαλί).


<img src="https://www.dropbox.com/s/9sput67pi0a4uqk/qrdqn-mlp-det.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 0.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-0.0
- Όπως παρατηρούμε και από τη γραφική, η ανταμοιβή από ένα σημείο και μετά συγκεντρώνεται στο 0.

#### Deterministic-v4 με Cnn-policy

- Έπειτα από μισό εκατομμύριο βήματα, πήραμε τα εξής αποτελέσματα.


<img src="https://www.dropbox.com/s/d2jx6xjg228nls3/qrdqn-cnn-det.png?dl=1" alt="2" border="0"></a>    

- **Μέση ανταμοιβή στο περιβάλλον ελέγχου (test): 0.0**  
- Τυπική απόκλιση της ανταμοιβής στο περιβάλλον ελέγχου (test): +/-0.0
- Παρατηρούμε πως σε κάθε περίπτωση ο QR-DQN καταλήγει σε μηδενική ανταμοιβή, συνεπώς δεν τον εξετάζουμε περαιτέρω.

#### Γενικά συμπεράσματα

Αν και όπως θα δούμε αργότερα στα `Leaderboards` ο αλγόριθμος αυτός καταφέρνει να πετύει ικανοποιητικά scores παιχνίδι μας, στη δική μας περίπτωση τα αποτελέσματα ήταν αρνητικά. Τόσο με MLP, όσο και με CNN πολιτική ο αλγόριθμος έδειχνε ανοδική πορεία μέχρι να φτάσει στο score `0`, το οποίο όμως έβλεπε σαν άνω φράγμα και παρά την εκπαίδευση που του κάναμε `(1M timesteps)` δεν κατάφερε να ξεπεράσει αυτό το score.

Για το λόγο αυτό και δεν συνεχίσαμε την εκπαίδευση και στη NonDeterministic και τη NonFrameskip εκδοχή του παιχνιδιού για το συγκεκριμένο.

### **Σύγκριση με state-of-the-art επιδόσεις**

Στο σημείο αυτό έχοντας κάνει τη δική μας "περιορισμένη" έρευνα ανατρέχουμε στα διαδικτυακά `leaderboards` του συγκεκριμένου παιχνιδιού για να δούμε που βρισκόμαστε συγκριτικά με τις state-of-the-art υλοποιήσεις διαφόρων αλγορίθμων. 

Σύμφωνα με την ιστοσελίδα [Papers with code: Atari Games Leaderboards](https://paperswithcode.com/sota/atari-games-on-atari-2600-private-eye) η καλύτερη υλοποίηση αυτή τη στιγμή για το παιχνίδι **Private Eye** φτάνει στο εξωπραγματικό για τα δεδομένα μας `95756` και γίνεται από τον αλγόριθμο `Go-Explore`.

Παρ'όλα αυτά αναζητώντας στην κατάταξη τους διαθέσιμους από την `sb3` αλγόριθμους συναντάμε πρώτο τον `DQN` σε μια εκδοχή με `Pixel-CNN` με την οποία καταφέρνει να πετύχει ένα score `8358.7`.

Σε γενικές γραμμές παρατηρούμε πως οι απλότερες υλοποιήσεις των διαθέσιμων από την `sb3` αλγορίθμων κυμαίνονται σε τιμές χαμηλότερες του `350` πράγμα ενθαρρυντικό, καθώς συγκριτικά με αυτές το αποτέλεσμά μας μοιάζει λογικό.

Συγκεκριμένα ο αλγόριθμος `PPO` ο οποίος χρησιμοποιήσαμε βρέθηκε να φτάνει στην απλή του μορφή την οποία και υλοποιήσαμε στο `105` το οποίο είναι ένα score συγκρίσιμο με τα δικά μας αποτελέσματα, σύμφωνα με τα leaderboards στο [endtoend.ai:Atari Private Eye Leaderboards](https://www.endtoend.ai/envs/gym/atari/private-eye/)

## Βέλτιστος πράκτορας

Ως βέλτιστο πράκτορα επιλέξαμε τον συνδυασμό του PPO αλγορίθμου, σε "απλό" περιβάλλον, χωρίς στοχαστικότητα(v4), και με cnn-policy. Παρακάτω, βρίσκονται μερικές πληροφορίες για τον αλγόριθμο PPO, καθώς και οι εντολές για επιτυχή φόρτωσή του:

#### Πληροφορίες

O PPO (Proximal Policy Optimization) είναι ένας Actor Critic αλγόριθμος. 

Αυτού του τύπου οι αλγόριθμοι στοχεύουν στο να συνδυάσουν τα πλεονεκτήματα των actor-only και των critic-only μεθόδων. Το critic κομμάτι τους προσπαθεί χρησιμοποιώντας μία αρχιτεκτονική εκτίμησης και προσομοίωσης να "μάθει" μία συνάρτηση αξίας, την οποία θα εκμεταλλευτεί έπειτα το actor κομμάτι ώστε να βελτιώσει την πολιτική που ακολουθεί στις κινήσεις του με σκοπό να "κινηθεί" έτσι ώστε να βελτιώσει την τιμή της συνάρτησης αξίας.

O PPO συγκεκριμένα είναι ένας αλγόριθμος κατασκευάστηκε από την ομάδα OpenAI το 2017. Η κύρια ιδέα πίσω από αυτόν είναι ότι συλλέγει ένα μικρό δείγμα από εμπειρίες αλληλεπιδρώντας με το περιβάλλον του και έπειτα χρησιμοποιεί το μικρό αυτό δείγμα για να ανανεώσει την στρατηγική λήψης αποφάσεων που έχει. Μετά το πέρας της ανανέωσης οι εμπειρίες αυτές "καταστρέφονται" και ένα νέο δείγμα από εμπειρίες συλλέγεται αυτή τη φορά με τη νέα στρατηγική λήψης αποφάσεων. Το γεγνός ότι το τρέον δείγμα επηρεάζει κάθε φορά μόνο την τρέχουσα ανενέωση παραμέτρων πολιτικής καθιστά τον αλγόριθμο αυτό έναν on-policy learning αλγόριθμο.

Ένα από τα βασικά επιτεύγματα του PPO είναι πως ο τρόπος με τον οποίο λειτουργεί εγγυάται πως η ανανεωμένη πολιτική δεν αλλάζει δραστικά συγκριτικά με την προηγούμενη πολιτική. Αυτό δημιουργεί μικρότερη διασπορά κατά την εκπαίδευση επιτρέποντάς της να είναι ομαλότερη, ενώ βεβαιώνει πως ο πράκτορας δεν θα χαθεί σε ένα μονοπάτι χωρίς επιστροφή λαμβάνοντας αποφάσεις χωρίς λογική.

#### Φόρτωση εκπαιδευμένου πράκτορα
"""

#ξαναβάζουμε τις απαραίτητες εντολές για να τρέξει αυτό το κελί

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.evaluation import evaluate_policy

!wget --no-check-certificate 'https://www.dropbox.com/s/duty07a55nc7y7b/ppo_private_eye_cnn_non_det_1p75M.zip?dl=1' -O ppo_private_eye_cnn_simple.zip

ppo_model = PPO.load("ppo_private_eye_cnn_simple", verbose=1)

atari_env_name='PrivateEye-v4'
new_env = make_atari_env(atari_env_name, n_envs=4)
new_env = VecFrameStack(new_env, n_stack=4)
ppo_model.set_env(new_env)

test_env = make_atari_env(atari_env_name, n_envs=1)
test_env = VecFrameStack(test_env, n_stack=4)

mean_reward, std_reward = evaluate_policy(ppo_model, test_env, n_eval_episodes=10)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

""" [Εδώ](https://www.dropbox.com/s/9496npapip75zs3/private_eye.mp4?dl=0) μπορέιτε να βρείτε το βίντεο με το gameplay του παιχνιδιού για τον συγκεκριμένο πράκτορα.

 Για ασφάλεια, το έχουμε επισυνάψει και στον παραδοτέο zip φάκελο.



"""