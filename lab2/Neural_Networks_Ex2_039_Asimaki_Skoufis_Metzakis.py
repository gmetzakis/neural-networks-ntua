# -*- coding: utf-8 -*-
"""NN_lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VtJ5Xs4dtU1dA9qd6trybJnD6LsXBfJT

# Εργαστηριακή Άσκηση 2. Μη επιβλεπόμενη μάθηση. 

Α/Α ομάδας: 39  
Ασημάκη Γεωργία Γρηγορία -- 03116197  
Μετζάκης Ιωάννης -- 03116202  
Σκούφης Πέτρος -- 03116141
"""

import pandas as pd
import scipy as sp
import joblib
import numpy as np

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer

"""## Εισαγωγή του Dataset"""

'''
dataset_url = "https://drive.google.com/uc?export=download&id=1PdkVDENX12tQliCk_HtUnAUbfxXvnWuG"
df_data_1 = pd.read_csv(dataset_url, sep='\t',  header=None, quoting=3, error_bad_lines=False)
joblib.dump(df_data_1, 'df_data_1.pkl')
''';

df_data_1 = joblib.load('df_data_1.pkl')

team_seed_number = 39

movie_seeds_url = "https://drive.google.com/uc?export=download&id=1EA_pUIgK5Ub3kEzFbFl8wSRqAV6feHqD"
df_data_2 = pd.read_csv(movie_seeds_url, header=None, error_bad_lines=False)

my_index = df_data_2.iloc[team_seed_number,:].values

titles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)
categories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)
bins = df_data_1.iloc[:, [4]]
catbins = bins[4].str.split(',', expand=True).values.astype(np.float)[my_index] # movie categories in binary form (1 feature per category)
summaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)
corpus = summaries[:,0].tolist() # list form of summaries

"""# Εφαρμογή 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο  

## Μετατροπή σε TFIDF

Αρχικά μετατρέπουμε το corpus σε αναπαράσταση tf-idf, χωρίς όμως να βελτιστοποιήσουμε τη συνάρτηση *TfidfVectorizer*
"""

vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
corpus_tf_idf = vectorizer.transform(corpus)
vocab = vectorizer.vocabulary_


corpus_tf_idf_array = corpus_tf_idf.toarray()
print(corpus_tf_idf_array.shape)
vocab

"""Παρατηρούμε πως το λεξιλόγιο που προκύπτει είναι αρκετά μεγάλο, πλήθους 49148 χαρακτηριστικών.

Στη συνέχεια υλοποιούμε τη συνάρτηση *content_recommender* και μέσω αυτής προσπαθούμε να βελτιστοποιήσουμε την *TfidVectorizer*. Για την υλοποίηση χρησιμοποιήσαμε την απόσταση συνημιτόνου.
"""

def content_recommender(target_movie, max_recommendations):
    
    distances = np.zeros((corpus_tf_idf.shape[0],2))

    for i in range(corpus_tf_idf_array.shape[0]):
            distances[i][0] = (sp.spatial.distance.cosine(corpus_tf_idf_array[i],corpus_tf_idf_array[target_movie]))
            distances[i][1] = i
    
    distances_sorted = np.array(sorted(distances, key= lambda x: x[0]))
    ID_sorted = distances_sorted[:,1]
    ID_sorted = np.delete(ID_sorted, 0)    #to 1o einai h target
    
    
    
    print("Ταινία Στόχος: ",'\n')
    print("ID: ", target_movie)
    print("Τίτλος: ", titles[target_movie])
    print("Σύνοψη: ", corpus[target_movie])
    print("Κατηγορίες: ", categories[target_movie], '\n\n\n')
    
    print("Συστάσεις:", '\n')
    
    for i in range(max_recommendations):
        ID = int(ID_sorted[i])
        
        print("%s."%(i+1))
        print("ID: ", ID)
        print("Τίτλος: ", titles[ID])
        print("Σύνοψη: ", corpus[ID])
        print("Κατηγορίες: ", categories[ID], '\n\n')

"""Για τη βελτιστοποίηση της συνάρτησης *TfidVectorizer*, η πρώτη σκέψη ήταν να εντοπίσουμε τις λέξεις, οι οποίες εμφανίζονται με μεγάλη συχνότητα στις περιλήψεις των ταινιών, αλλά στην ουσία δεν προσδίδουν τίποτα στο σημασιολογικό περιεχόμενό τους, και να τις προσθέσουμε στην παράμετρο "*stop_words*" της *TfidVectorizer*.

Αρχικά, χρησιμοποιήσαμε ως stop_words την έτοιμη λίστα text.ENGLISH_STOP_WORDS. Στο λεξιλόγιο που προέκυψε, προβάλαμε τα πρώτα 50 στοιχεία (ταξινομημένα σε φθίνουσα συχνότητα στο κείμενο), επιλέξαμε κάποιες λέξεις που θεωρήσαμε ότι δεν έχουν σχέση με το περιεχόμενο των ταινιών, και τις προσθέσαμε στη λίστα stop_words.

Οι λέξεις αυτές είναι οι εξής: 
*['film','director','gets','finally','end','goes','new','old','story','took','takes','access','accessed','review']*, καθώς και οι αριθμοί *['one', ..., 'ten']*.

Φυσικά και υπάρχουν αρκετές ακόμα λέξεις που μπορούν να προστεθούν στην παραπάνω λίστα, ωστόσο επιλέξαμε αυτές που εμφανίζονται περισσότερο στο συνολικό dataset.

Στη συνέχεια, μέσα από δοκιμές εστιάσαμε στις συστάσεις που είχαν σημασιολογικά μεγάλη διαφορά από την εκάστοτε ταινία-στόχο, και προσπαθήσαμε να εντοπίσουμε το λόγο που συμβαίνει αυτό.

Συγκεκριμένα, για τις ταινίες με ID=62, ID=1350, ID=1555 παρατηρήσαμε πως οι συστάσεις βασιζόταν κυρίως στο κοινό όνομα Chris, Lloyd και Hal αντίστοιχα.
"""

content_recommender(62,1)
content_recommender(1350,1)
content_recommender(1555,1)

"""Για την επίλυση του συγκεκριμένου προβλήματος, αρχικά εξετάσαμε τη μείωση της διαστατικότητας.

Ωστόσο, ορισμένα ονόματα εμφανίζονται σε πολλές ταινίες, και ενώ η υπερβολική αύξηση της παραμέτρου "*min_df*" της *TfidVectorizer* μπορεί να έλυνε το πρόβλημα αυτό, θα δημιουργούσε άλλα προβλήματα, καθώς θα χάναμε λέξεις που όντως συνδέουν σημασιολογικά ένα μικρότερο σύνολο ταινιών. 

Συνεπώς, σκεφτήκαμε να προσθέσουμε ένα μεγάλο αριθμό μικρών ονομάτων στη λίστα stop_words.
Για το σκοπό αυτό, βρήκαμε ένα dataset με τα 5162 περισσότερο γνωστά ονόματα στα αγγλικά, και μετά από κατάλληλη επεξεργασία τα προσθέσαμε στη λίστα stop_words.
"""

data_names0 = pd.read_csv("us.csv")
data_names = data_names0.to_numpy()
names = []
for i in range(len(data_names)):
    names.append(data_names[i][0].lower())

numbers = ['one','two','three','four','five','six','seven','eight','nine','ten']
words = ['film','director','gets','finally','end','goes','new','old','story','took','takes','access','accessed','review']

my_stop_words = text.ENGLISH_STOP_WORDS.union(words).union(names).union(numbers)

vectorizer = TfidfVectorizer(stop_words=my_stop_words)
vectorizer.fit(corpus)
corpus_tf_idf = vectorizer.transform(corpus)
vocab = vectorizer.vocabulary_

corpus_tf_idf_array = corpus_tf_idf.toarray()
print(corpus_tf_idf_array.shape)
vocab

"""Έπειτα, συνεχίζοντας την πειραματική διαδικασία εντοπισμού πιθανών σφαλμάτων, διαπιστώσαμε πως για την ταινία-στόχο με ID=1481 η μόνη σχέση της με την 2η σύσταση είναι η χρονολογία 2009, χωρίς όμως αυτή να σχετίζεται με το περιεχόμενο της ταινίας."""

content_recommender(1481,2)

"""Αυτό μας οδηγεί στη σκέψη να αφαιρέσουμε όλα τα χαρακτηριστικά που αποτελούνται από ψηφία. 
Γνωρίζουμε το ρίσκο του να επηρεαστούν οι συστάσεις σε ταινίες που αυτό που τις συνδέει είναι η χρονολογία, όπως για παράδειγμα σχετικές με συγκεκριμένους πολέμους, ή ιστορικά γεγονότα. Παρόλα αυτά, θεωρήσαμε πως το κέρδος είναι μεγαλύτερο, καθώς αφαιρούνται όλα τα προβλήματα όπως του παραπάνω παραδείγματος (με το 2009), ενώ επίσης οι ταινίες που αναφέραμε είναι αρκετά πιθανό να συνδέονται και με άλλες λέξεις κλειδιά, όπως nazi, vietnam, κ.α.

Για την υλοποίηση λοιπόν, δημιουργήσαμε ξανά το αρχικό λεξιλόγιο, και τυπώσαμε τα στοιχεία ταξινομημένα αλφαβητικά, αφού τα ψηφία ταξινομούνται πριν από κάθε γράμμα. Εντοπίσαμε πως τα πρώτα 724 χαρακτηριστικά αποτελούνται από ψηφία, συνεπώς τα προσθέσαμε στη λίστα stop_words, όπου είναι και η τελική.

Παρακάτω, η διαδικασία υπολογισμού των πρώτων αυτών στοιχείων:
"""

vectorizer = TfidfVectorizer()
vectorizer.fit(corpus)
corpus_tf_idf = vectorizer.transform(corpus)
vocab = vectorizer.vocabulary_

corpus_tf_idf_array = corpus_tf_idf.toarray()

ff = vectorizer.get_feature_names() #arithmoi oti na nai
first_feats0 = ff[:724]
first_feats = [x.lower() for x in first_feats0]
print(first_feats)
#joblib.dump(first_feats,'first_feats.pkl')

"""Έχοντας πλέον την τελική λίστα stop_words, μπορούμε να πειραματιστούμε με την παράμετρο "*min_df*" της *TfidVectorizer* για να μειώσουμε τη διαστατικότητα. 

Η συγκεκριμένη παράμετρος, όταν δεχθεί όρισμα έναν θετικό ακέραιο αριθμό n, λειτουργεί ως threshold, απορρίπτοντας όλες τις λέξεις που εμφανίζονται σε λιγότερες από n ταινίες του dataset.

Μετά από αρκετές δοκιμές, αποφασίσαμε να τη θέσουμε ίση με 5.

Η επιλογή του συγκεκριμένου αριθμού έγινε με βάση τα εξής:

Διαπιστώσαμε πως για χαμηλότερο min_df, μπορεί να είχαμε εξαιρετικά αποτελέσματα για μικρό αριθμό ζητούμενων συστάσεων, αφού θα υπολογιζόταν πολύ συγκεκριμένες λέξεις, όπως χαρακτήρες ή μέρη που υπάρχουν σε τριλογίες κλπ, ωστόσο για μεγαλύτερο αριθμό συστάσεων θα ξεφεύγαμε αρκετά από σημασιολογικής πλευράς.

Αντίστοιχα, για μεγαλύτερο min_df μπορεί να είχαμε μία πιο γενική, αλλά σταθερή συσχέτιση μεταξύ των συστάσεων, χάναμε όμως κάποιες συστάσεις που θεωρήσαμε ως απαραίτητες.


# Τελικό TFIDF

Παρακάτω βρίσκεται το τελικό TFIDF (12164 χαρακτηριστικά), όπως αυτό προέκυψε από τις επιλογές που εξηγήσαμε προηγουμένως.

<sub>\*Προφανώς μπορούμε να τρέξουμε κατευθείαν το παρακάτω cell. Όλα τα παραπάνω παρουσιάστηκαν απλά για να φανεί η διαδικασία που ακολουθήσαμε μέχρι να προκύψει το τελικό.</sub>
"""

first_feats = joblib.load('first_feats.pkl')

data_names0 = pd.read_csv("us.csv")
data_names = data_names0.to_numpy()
names = []
for i in range(len(data_names)):
    names.append(data_names[i][0].lower())

numbers = ['one','two','three','four','five','six','seven','eight','nine','ten']
words = ['film','director','gets','finally','end','goes','new','old','story','took','takes','access','accessed','review']

my_stop_words = text.ENGLISH_STOP_WORDS.union(words).union(names).union(numbers).union(first_feats)

vectorizer = TfidfVectorizer(stop_words=my_stop_words, min_df=5)
vectorizer.fit(corpus)
corpus_tf_idf = vectorizer.transform(corpus)
vocab = vectorizer.vocabulary_

corpus_tf_idf_array = corpus_tf_idf.toarray()
print(corpus_tf_idf_array.shape)
vocab

"""# Παραδείγματα καλής λειτουργίας του συστήματος συστάσεων

1. Για την ταινία με ID=1059, παρατηρούμε πως συνδέεται άμεσα με όλες τις συστάσεις, καθώς όλες περιέχουν zombies.
"""

content_recommender(1059,5)

"""2. Η την ταινία με ID=2351, συνδέεται με τις συστάσεις της μέσω της κοινής θεματικής των εξωγήινων."""

content_recommender(2351,5)

"""3. Για την ταινία με ID=1120 (Tarzan), βλέπουμε πως αρχικά συστήνει όλες τις υπόλοιπες ταινίες με τον Tarzan, ενώ η 5η σύσταση συνδέεται μέσω της θεματικής των πιθήκων (ape man)."""

content_recommender(1120,5)

"""4. Για την ταινία με ID=1555, βλέπουμε πως συνδέεται με όλες τις συστάσεις μέσω της θεματικής του διαστήματος και του πολέμου, ενώ όλες είναι ταινίες επιστημονικής φαντασίας."""

content_recommender(1555,5)

"""5. Η ταινία με ID=62 συνδέεται με τις συστάσεις της μέσω της θεματικής του baseball."""

content_recommender(62,5)

"""6. Για την ταινία με ID=1137 (Scooby-Doo), παίρνουμε αρχικά ως συστάσεις τα υπόλοιπα Scooby-Doo, ενώ οι επόμενες συστάσεις συνδέονται μέσω της θεματικής των εξωγήινων."""

content_recommender(1137,5)

"""7. Η ταινία με ID=2534 και οι συστάσεις της έχουν κοινή θεματική τα Χριστούγεννα."""

content_recommender(2534,5)

"""8. H ταινία με ID=1481 και οι συστάσεις της έχουν κοινή θεματική τα παράξενα πλάσματα-τέρατα."""

content_recommender(1481,5)

"""9. H ταινία με ID=2883 και οι συστάσεις της έχουν κοινή θεματική τις δολοφονίες."""

content_recommender(2883,5)

"""10. H ταινία με ID=4029 και οι συστάσεις της έχουν κοινή θεματική τα βαμπίρ, τον δράκουλα, το αίμα."""

content_recommender(4029,5)

"""Δοκιμάσαμε να κάνουμε και μία PCA, αλλά ενώ μειώθηκε αισθητά η διαστατικότητα (στα 3000 χαρακτηριστικά), δεν είχαμε βελτίωση των συστάσεων, αντιθλετως μάλιστα παρατηρήσαμε πολλά "άσχετα" αποτελέσματα."""

'''##PCA
from sklearn.decomposition import PCA

pca = PCA(0.97, whiten=True)
vocab_pca = pca.fit_transform(corpus_tf_idf_array)
vocab_pca.shape
''';

"""# Εφαρμογή 2.  Τοπολογική και σημασιολογική απεικόνιση της ταινιών με χρήση SOM
<img src="https://i.imgur.com/Z4FdurD.jpg" width="60%">

## Δημιουργία dataset
Στη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους (ο παραπάνω χάρτης είναι ενδεικτικός, δεν αντιστοιχεί στο dataset μας). 

Η `build_final_set` αρχικά μετατρέπει την αραιή αναπαράσταση tf-idf της εξόδου της `TfidfVectorizer()` σε πυκνή (η [αραιή αναπαράσταση](https://en.wikipedia.org/wiki/Sparse_matrix) έχει τιμές μόνο για τα μη μηδενικά στοιχεία). 

Στη συνέχεια ενώνει την πυκνή `dense_tf_idf` αναπαράσταση και τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον στήλες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά του TFIDF και τις κατηγορίες της.

Τέλος, δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM.
"""

corpus_tf_idf = joblib.load('corpus_tf_idf_metz.pkl')

def build_final_set(doc_limit = 5000, tf_idf_only=False):
    # convert sparse tf_idf to dense tf_idf representation
    dense_tf_idf = corpus_tf_idf.toarray()[0:doc_limit,:]
    if tf_idf_only:
        # use only tf_idf
        final_set = dense_tf_idf
    else:
        # append the binary categories features horizontaly to the (dense) tf_idf features
        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))
        # η somoclu θέλει δεδομένα σε float32
    return np.array(final_set, dtype=np.float32)

final_set = build_final_set()

"""Τυπώνουμε τις διαστάσεις του τελικού dataset μας. Χωρίς βελτιστοποίηση του TFIDF θα έχουμε περίπου 50.000 χαρακτηριστικά."""

final_set.shape

"""Με βάση την εμπειρία σας στην προετοιμασία των δεδομένων στην επιβλεπόμενη μάθηση, υπάρχει κάποιο βήμα προεπεξεργασίας που θα μπορούσε να εφαρμοστεί σε αυτό το dataset;

Από τη στιγμή που θέλουμε να προσχωρήσουμε σε μείωση της διαστατικότητας θα μπορούσαμε να χρησιμοποιήσουμε την ανάλυση PCA επιλέγοντας να διατηρηθεί ένα μεγάλο ποσοστό της αρχικής διακύμανσης των δεδομένων. Προχωρήσαμε σε κάτι τέτοιο από το προηγούμενο μέρος της άσκησης μειώνοντας τη διαστατικότητα σε 3400 χαρακτηριστικά **ΑΛΛΑ** ελέγχοντας τις συστάσεις που παρήγαγε το σύστημά μας μετά την PCA παρατηρήσαμε πως υστερούν και επιλέξαμε για το λόγο αυτό να προχωρήσουμε με το dataset **ΧΩΡΙΣ** PCA.

## Εκπαίδευση χάρτη SOM

Θα δουλέψουμε με τη βιβλιοθήκη SOM ["Somoclu"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window).
"""

# Commented out IPython magic to ensure Python compatibility.
# install somoclu
!pip install --upgrade somoclu
!pip install --upgrade networkx
!pip install --upgrade plotly
# import sompoclu, matplotlib
import somoclu
import matplotlib 
import matplotlib.pyplot as plt
import networkx as nx
import plotly.graph_objects as go
# we will plot inside the notebook and not in separate window
# %matplotlib inline

"""Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. Ενδεικτικά, με σωστή κωδικοποίηση tf-idf, μικροί χάρτες για λίγα δεδομένα (1000-2000) παίρνουν γύρω στο ένα λεπτό ενώ μεγαλύτεροι χάρτες με όλα τα δεδομένα μπορούν να πάρουν 10-15 λεπτά ή και περισσότερο.

Παρακάτω φαίνεται ένα παράδειγμα της διαδικασίας που ακολουθήσαμε για τη δημιουργία των χαρτών. Επιλέξαμε να αποθηκεύουμε τους χάρτες πριν κάνουμε το clustering, για να μπορούμε να πειραματιστούμε πάνω σε αυτό. Επίσης επιλέξαμε για το μεγαλύτερο μέρος να ασχοληθούμε με υποσύνολα των ταινιών (2000).
"""

# Commented out IPython magic to ensure Python compatibility.
n_columns, n_rows = 30,30
som = somoclu.Somoclu(n_columns, n_rows, maptype='planar',gridtype = 'rectangular',compactsupport=False)
data=build_final_set(doc_limit = 2000)
# %time som.train(data,epochs=100)

joblib.dump(som, 'som_30.pkl')

#Run cell to load an existing pickle
#som = joblib.load('som_30.pkl')
som = joblib.load('som25_full.pkl')

"""
## Best matching units

Μετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.
"""

bmus = som.bmus
print(bmus.shape)
#monadika bmus
ubmus, indices = np.unique(bmus, return_inverse=True, axis=0)
print(ubmus)
print(indices)

"""
## Ομαδοποίηση (clustering)

Τυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.

To somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.
"""

from sklearn.cluster import KMeans
algorithm = KMeans(n_clusters=35)
som.cluster(algorithm=algorithm)

#joblib.dump(som, 'som.pkl')

"""## Αποθήκευση του SOM

Επειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.

## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters

Για την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.

Για μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.

Τέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. 

Ακολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:

<img src="https://image.ibb.co/i0tsfR/umatrix_s.jpg" width="35%">
<img src="https://image.ibb.co/nLgHEm/clusters.png" width="35%">

Παρακάτω ακολουθεί η υλοποίησή μας για το συγκεκριμένο τμήμα.
"""

from numpy import argsort,unique
som.view_umatrix(bestmatches=True,figsize=(20,20),colorbar=True)
print(som.clusters)
cl, i ,count= unique(som.clusters,return_inverse=True,return_counts=True)
print("Clusters sorted by increasing number of neurons")
print("Cluster index")
print("Number of neurons")
sorted=argsort(count)
a=np.zeros((2,25),dtype=int)
for j in range(25):
  a[0][j]=sorted[j]
  a[1][j]=count[sorted[j]]
print(a)

"""## Σημασιολογική ερμηνεία των clusters

Προκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης με το tf-idf και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.

Μπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:

1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.

2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.

3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.

Μπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.

Θα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. 

Παράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):

```
Overall Cluster Genres stats:  
[('"Horror"', 86), ('"Science Fiction"', 24), ('"B-movie"', 16), ('"Monster movie"', 10), ('"Creature Film"', 10), ('"Indie"', 9), ('"Zombie Film"', 9), ('"Slasher"', 8), ('"World cinema"', 8), ('"Sci-Fi Horror"', 7), ('"Natural horror films"', 6), ('"Supernatural"', 6), ('"Thriller"', 6), ('"Cult"', 5), ('"Black-and-white"', 5), ('"Japanese Movies"', 4), ('"Short Film"', 3), ('"Drama"', 3), ('"Psychological thriller"', 3), ('"Crime Fiction"', 3), ('"Monster"', 3), ('"Comedy"', 2), ('"Western"', 2), ('"Horror Comedy"', 2), ('"Archaeology"', 2), ('"Alien Film"', 2), ('"Teen"', 2), ('"Mystery"', 2), ('"Adventure"', 2), ('"Comedy film"', 2), ('"Combat Films"', 1), ('"Chinese Movies"', 1), ('"Action/Adventure"', 1), ('"Gothic Film"', 1), ('"Costume drama"', 1), ('"Disaster"', 1), ('"Docudrama"', 1), ('"Film adaptation"', 1), ('"Film noir"', 1), ('"Parody"', 1), ('"Period piece"', 1), ('"Action"', 1)]```

Παρακάτω ακολουθεί η υλοποίησή μας για τις συναρτήσεις της εκφώνησης. Σημειώνεται πως αυτές είναι τροποποιημένες ώστε είτε να τυπώνουν πληροφορίες είτε να επιστρέφουν πληροφορίες για χρήση από άλλες συναρτήσεις.
"""

from numpy import ravel
from numpy import split

def print_categories_stats(movie_ids=[], printer = True):
    allCategories=[]
    for i in range(len(movie_ids)):
        ID=movie_ids[i]
        c=np.array2string(categories[ID])
        c=c.translate({ord("'"): None})
        #print(c,'\n')
        cat=c.split(', ')
        #print(cat,'\n')
        for j in range(len(cat)):
            x=cat[j]
            y=x.strip()
            y=y.translate({ord('['): None})
            #y=y.translate({ord('\\'): ""})
            cat[j]=y.translate({ord(']'): None})
        #print(cat, '\n\n\n')
        allCategories.extend(cat)
    clust, index, freq = np.unique(allCategories,return_inverse=True,return_counts=True)
    #sort = np.argsort(freq)
    #s = sort[::-1]
    #z = zip(clust[s],freq[s])
    #print(set(z))
    
    z = zip(clust,freq)
    zipped=list(z)
    zipped.sort(key = lambda x: x[1], reverse=True)
    if printer == True:
      print(zipped)
    else:
      return zipped[:3]

def print_cluster_neurons_movies_report(cluster_id, printer = True):
  x, y = np.where(som.clusters == cluster_id)
  coord = np.column_stack((y,x))
  neurons_idx = [] 
  for row in coord:
    new_neuron = np.where(np.all(ubmus == row, axis=1))
    if new_neuron[0].shape[0] == 1:
      neurons_idx.extend(new_neuron[0])
  if printer == True : 
    neuron_movies_report(neurons_idx)
  else:
    return neurons_idx , coord

def neuron_movies_report(neurons, printer = True):
  movies = []
  for n in neurons: 
    idx = np.where(indices == n)
    movies.extend(idx[0].tolist())
  #print(len(movies))
  if printer == True:
    print_categories_stats(movies)
  else: 
    return movies

print_cluster_neurons_movies_report(14)

"""Η παρακάτω συνάρτηση αποτελεί ένα τρόπο για συμπαγή ανάλυση των χαρακτηριστικών του κάθε cluster και γρήγορη επισκόπηση σημαντικών πληροφοριών, όπως οι νευρώνες που ανατέθηκαν στον cluster και η συχνότητα εισόδου."""

def total_cluster_comp(total_clusters):
  clust_lst = [] 
  for i in range(total_clusters):
    clust_neurons, _ = print_cluster_neurons_movies_report(i, False)
    clust_movies = neuron_movies_report(clust_neurons, False)
    clust_top_features = print_categories_stats(clust_movies, False)
    clust_lst.append([i, len(clust_neurons), len(clust_movies), clust_top_features])
  df = pd.DataFrame(clust_lst, columns = ['Cluster ID','Total Neurons','Total Movies','Top 3 Categories'])
  return df

test = total_cluster_comp(25)

test

"""Παρακάτω ακολουθούν βοηθητικές συναρτήσεις που έχουν ως στόχο την οπτικοποίηση του γράφου γειτνίασης των clusters."""

def get_neighbors_neuron(x,y):
  neighbors_arr = som.clusters[(x-1):(x+2),(y-1):(y+2)]
  neighbors = np.unique(neighbors_arr)
  return neighbors.tolist()

def get_neighbors_cluster(id):
  cluster_neighbors = set([])
  _,neurons = print_cluster_neurons_movies_report(id, False)
  for n in neurons:
    n_set = set(get_neighbors_neuron(n[1],n[0]))
    cluster_neighbors = cluster_neighbors.union(n_set)
  return list(cluster_neighbors)

def get_neighbors_list(total_clusters):
  neighbors = []
  for i in range(total_clusters):
    cluster_neighbors = get_neighbors_cluster(i)
    neighbors.append(cluster_neighbors)
  return neighbors

def plot_topology0(total_clusters):
  G = nx.Graph()
  G.add_nodes_from([i for i in range(total_clusters)])
  neighbors = get_neighbors_list(total_clusters)
  for i in range(len(neighbors)):
    for j in neighbors[i]:
      G.add_edge(i,j)
  plt.figure(figsize= (20,20))
  plt.subplot(111)
  nx.draw(G, with_labels=True, font_weight='bold')

test = total_cluster_comp(35)

top3 = test['Top 3 Categories'].to_list()

top3final = []
for i in range(len(top3)):
    top3final.append(top3[i][0][0]+','+top3[i][1][0]+','+top3[i][2][0])
top3 = top3final
#print(top3)

import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots

#import plotly.offline as pyo
#pyo.init_notebook_mode(connected=False)    


def plot_topology(total_clusters,top3):
    G = nx.Graph()
    G.add_nodes_from([i for i in range(total_clusters)])
    neighbors = get_neighbors_list(total_clusters)
    for i in range(len(neighbors)):
        for j in neighbors[i]:
            if (i!=j):
                G.add_edge(i,j)
    
    pos = nx.spring_layout(G, k=0.5)
    nx.set_node_attributes(G,pos,'pos')
    
    edge_x = []
    edge_y = []
    xtext=[]
    ytext=[]
    for edge in G.edges():
        x0, y0 = G.nodes[edge[0]]['pos']
        x1, y1 = G.nodes[edge[1]]['pos']
        xtext.append((x0+x1)/2)
        ytext.append((y0+y1)/2)
        edge_x.append(x0)
        edge_x.append(x1)
        edge_x.append(None)
        edge_y.append(y0)
        edge_y.append(y1)
        edge_y.append(None)

    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        showlegend=False,
        line=dict(width=0.5, color='#888'),
        hoverinfo='text',
        mode='lines', 
        xaxis='x2', yaxis='y2')
    
    connections_trace = go.Scatter(x=xtext,y= ytext, 
        mode='markers',          
        hoverinfo='text',
        showlegend=False,
        marker=dict(
            showscale=False,
            colorscale='Greys',
            reversescale=False,
            color=[],
            size=0.2),
            xaxis='x2', yaxis='y2')
    

    node_x = []
    node_y = []
    for node in G.nodes():
        x, y = G.nodes[node]['pos']
        node_x.append(x)
        node_y.append(y)

    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        hoverinfo='text',
        showlegend=False,
        marker=dict(
            showscale=False,
            colorscale='YlGnBu',
            reversescale=True,
            color='darkviolet',
            size=20,
            line_width=2), 
            xaxis='x2', yaxis='y2')
    
    
    node_adjacencies = []
    node_text = []
    node_t = []
    for node, adjacencies in enumerate(G.adjacency()):
        node_adjacencies.append(7)
        node_text.append(top3[node])
        node_t.append(node)
            
    node_trace.marker.color = node_adjacencies
    node_trace.text = node_t
    node_trace.hovertext = node_text

    edges_connected = []
    for edges in G.edges():
        edges_connected.append('%s --- %s' %(top3[edges[0]],top3[edges[1]]))
    connections_trace.text = edges_connected
    
    
    
    

    fig = make_subplots(
        rows=1, cols=2,
        column_widths=[0.7, 0.5],
        shared_yaxes=True,
        horizontal_spacing = 0.005,
        specs=[[{"secondary_y": True}, {'type':'table'}]]
    )
    
    fig.add_trace(edge_trace, row=1, col=1)
    fig.add_trace(node_trace, row=1, col=1)
    fig.add_trace(connections_trace, row=1, col=1)

    fig.add_trace(go.Table(
        columnwidth = [1,5],
        header=dict(
            values=["Number", "TOP 3"],
            font=dict(size=10),
            align="left"
        ),
        cells=dict(
            values=[[i for i in range(len(top3))],[top3[i] for i in range(len(top3))]],
            align = "left")), row=1,col=2)
    
    fig.update_layout(height=600, width=1200)
                    
    
    fig.show()
    #pyo.plot(fig)

"""## Tips για το SOM και το clustering

- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix.
- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.
- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.
- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.

## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM

Μετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook (η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας).

Για τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: 

1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.
2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.
3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.

Προφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.

Εντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.



Τέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.

### Εισαγωγή στον τρόπο ανάλυσης

Αφού ολοκληρώσαμε τη διαδικασία της εκπαίδευσης και του clustering, επιλέξαμε να προβάλουμε τα αποτελέσματα με δύο διαφορετικούς τρόπους οπτικοποίησης, πέραν του `u_matrix` που υπάρχει στη βιβλιοθήκη. 

Η **πρώτη** οπτικοποίηση χρησιμοποιεί τη βιβλιοθήκη `pandas` με την οποία προβάλουμε ένα `DataFrame` που περιέχει σημαντικές πληροφορίες για τα clusters που δημιούργησε ο αλγόριθμός μας. Συγκεκριμένα προβάλουμε:

* `Cluster ID` : το χαρακτηριστικό αριθμό που αντιστοιχεί στο κάθε cluster.
* `Total Neurons` : το σύνολο σε πλήθος των νευρώνων που έχουν ανατεθεί σε αυτό το cluster.
* `Total Movies` : το σύνολο των ταινιών που κατηγοριοποιήθηκαν τελικά σε αυτό το cluster.
* `Top 3 Categories` : τους τίτλους των 3 κορυφαίων κατηγοριών που ανατέθηκαν στο συγκεκριμένο cluster, όσον αφορά τη συχνότητα εμφάνισής τους.

Η **δεύτερη** οπτικοποίηση χρησιμοποιεί τις βιβλιοθήκες `networkx` και `plotly` . Στόχος της οπτικοποίησης είναι η απεικόνιση των clusters σε μορφή γράφου, ώστε να γίνουν πιο εμφανείς οι σχέσεις γειτνίασης μεταξύ τους. 

Με τη `networkx` δημιουργούμε έναν απλό γράφο, όπου ο κάθε κόμβος αντιστοιχεί σε ένα cluster, του οποίου την ετικέτα και φέρει. Η ύπαρξη ακμής μεταξύ δύο κόμβων σηματοδοτεί τη γειτνίαση αυτών των δύο κόμβων. Η βιβλιοθήκη αυτή μας επιτρέπει μια πρώτη οπτικοποίηση, δε συγκεντρώνει όμως μεγάλο βαθμό πληροφορίας και συνίσταται η χρήση της σε συνδυασμό με το `pandas` Dataframe που αναπτύξαμε παραπάνω.

Αντίθετα, η υλοποίησή μας με χρήση της **διαδραστικής** `plotly` μάς επιτρέπει να λάβουμε επιπλέον πληροφορίες από το γράφο πέρα από αυτές της `networkx`. Με την επέκταση αυτή λοιπόν έχουμε τη δυνατότητα:
* Να δούμε άμεσα τις `'Top 3 Categories'` του συγκεκριμένου cluster, κάνοντας **hover** το ποντίκι μας πάνω από ένα συγκεκριμένο node.
* Να κάνουμε zoom-in σε ένα συγκεκριμένο τμήμα του γράφου, λειτουργία πολύ χρήσιμη σε περιπτώσεις με πολλά clusters.

Ο λόγος που τελικά χρησμιποιήσαμε και τις δύο ενώ φανερά η δεύτερη υλοποίηση του γράφου είναι καλύτερη είναι η έλλειψη δυνατότητας αποθήκευσης της κατάστασης των γραφημάτων της `plotly` από το notebook, με εξαίρεση την εισάγωγή τους μέσω `HTML`

### Πειραματισμός και Συμπεράσματα στο SOM Clustering
"""

from numpy import argsort,unique
import numpy as np
from sklearn.cluster import KMeans

def som_algorithm(som,clusters):
    algorithm = KMeans(n_clusters=clusters)
    som.cluster(algorithm=algorithm)
    som.view_umatrix(bestmatches=True,figsize=(20,20),colorbar=True)
    print(som.clusters)
    cl, i ,count= unique(som.clusters,return_inverse=True,return_counts=True)
    print("Clusters sorted by increasing number of neurons")
    print("Cluster index")
    print("Number of neurons")
    sorted=argsort(count)
    a=np.zeros((2,10),dtype=int)
    for j in range(10):
        a[0][j]=sorted[j]
        a[1][j]=count[sorted[j]]
    print(a)

"""Αρχικά, θα κατασκευάσουμε μικρούς χάρτες SOΜ σε ένα δείγμα 1000 ταινιών, διαστάσεων 10 Χ 10, και 15 clusters στον Κ-means, δοκιμάζοντας διαφορετικές παραμέτρους για την εκπαίδευση του SOM.

Συγκεκριμένα, θα πειραματιστούμε με τις παραμέτρους "*maptype*" και "*compactsupport*", διατηρώντας την παράμετρο `gridtype = 'rectangular'`, αφού για `gridtype = 'hexagonial'` δεν μπορούσαμε να εξάγουμε χρήσιμα συμπεράσματα.
"""

som_list = ['som_10_1000_pl_rec.pkl',
           'som_10_1000_tor_rec.pkl',
           'som_10_1000_pl_rec_true.pkl']

for i in [0,2]:
    som = joblib.load(som_list[i])
    som_algorithm(som,15)

"""Παρατηρούμε πως όταν `compactsupport = 'False'` εμφανίζονται πιο έντονα οι περιοχές ορίων, ενώ για `compactsupport = 'True'`, εμφανίζονται περισσότερο οι περιοχές χαμηλής απόστασης."""

for i in [0,1]:
    som = joblib.load(som_list[i])
    som_algorithm(som,15)

"""Παρομοίως, για `maptype = 'planar'` έχουμε αρκετά έντονες τις περιοχές ορίων, ενώ για `maptype = 'toroid'` εμφανίζονται και περιοχές χαμηλής απόστασης.

Τελικά, αποφασίσαμε να χρησιμοποιήσουμε τις αρχικές παραμέτρους για την εκπαίδευση όλων των SOM που θα μελετήσουμε στη συνέχεια, δηλαδή `maptype = 'planar'`, `gridtype = 'rectangular'` και `compactsupport = 'False'`.



Στο σημείο αυτό, θα κατασκευάσουμε χάρτες SOM διαφορετικών διαστάσεων για final sets που περιέχουν 2000 ταινίες και θα σχολιάσουμε τα χαρακτηριστικά τους.

Ξεκινάμε από έναν μικρό χάρτη με grid 10 X 10 και 10 clusters στον K_means
"""

som = joblib.load('som_10.pkl')
som_algorithm(som,10)

"""Παρατηρούμε πως ήδη διακρίνονται οπτικά στο χάρτη τα πρώτα clusters, τα οποία βρίσκονται είτε σε περιοχές μικρής απόστασης (μπλε - πράσινα - κίτρινα) σημεία που περικλείονται από κόκκινα όρια, είτε στα κόκκινα όρια. Επίσης με τη βοήθεια του παρακάτω πίνακα που απεικονίζει τον αριθμό των ταινιών στο κάθε cluster παρατηρούμε πως τα μεγάλα σε μέγεθος grids είναι αυτά που έχουν και πολλές ταινίες στο χώρο εισόδου."""

total_cluster_comp(10)

"""Για τις ίδιες διαστάσεις χάρτη θα αυξήσουμε τώρα τον αριθμό των clusters του K-Means σε 15."""

som_algorithm(som,15)

"""Παρατηρούμε πως εσωτερικά των σημείων χαμηλής απόστασης τα clusters διατηρήθηκαν ίδια, ενώ στα σύνορα (περιοχές δηλαδή με μεγάλη απόσταση) ο αλγόριθμος ξεκίνησε να δημιουργεί νέες κατηγορίες. Αυτό είναι ένα σημάδι πως μπορούμε να αναζητήσουμε περισσότερες αρκετά διακριτές κατηγορίες με αύξηση των διαστάσεων του χάρτη."""

total_cluster_comp(15)

"""Από την ανάλυση του πίνακα με τα χαρακτηριστικά του περιεχομένου του κάθε cluster παρατηρούμε πως τα cluster που δημιουργούνται έχουν ξεκάθαρο περιεχόμενο και κατηγορίες που προηγουμένων αποτελούσαν μία κοινή πλέον σπάνε στις υποκατηγορίες τους. Παράδειγμα το cluster = 10 με κύριες κατηγορίες τις `Black and White` και `Silent Film` το οποίο πλέον έχει 42 ταινίες ενώ στην προηγούμενη εκτέλεση του k-means το αντίστοιχο με cluster_id = 2 είχε 90.

Παρατηρούμε όμως πως ο χάρτης είναι μικρός σε διαστάσεις για να αποτυπώσει με επιτυχία τα δεδομένα, οπότε θα δοκιμάσουμε μεγαλύτερους.
"""

som = joblib.load('som15.pkl')
som_algorithm(som,15)

"""Ο 15 Χ 15 προσθέτει περισσότερες διακριτές περιοχές χαμηλής απόστασης ενώ μειώνει δραστικά τα όρια μεταξύ τους. Παρατηρούμε όμως πως τα 15 clusters τα οποία θέσαμε στον K-Means δεν είναι αρκετά, καθώς δε σέβονται τα όρια των περιοχών, οπότε θα επαναλάβουμε το clustering για k = 25 αυτή τη φορά."""

som_algorithm(som,25)

"""Παρατηρούμε πως η αύξηση του K-Means περιορίζει τα clusters εντός των περιοχών του χάρτη, χωρίς ακόμα όμως να σέβονται απόλυτα την τοπολογία. Παρατηρώντας και παρακάτω την ανάλυση των χαρακτηριστικών των clusters βλέπουμε πως υπάρχουν clusters χωρίς τόσο σαφές περιεχόμενο, όπως το cluster = 2 και το cluster = 19."""

total_cluster_comp(25)

"""Ήδη από μικρούς χάρτες παρατηρούμε πως clusters που αναπαριστούν διαφορετικές κατηγορίες βρίσκονται αρκετά μακριά στο χάρτη. Συγκεκριμένα, το cluster = 0 και το cluster = 11 που αναπαριστούν κωμωδίες και θρίλερ βρίσκονται αντιδιαμετρικά στο χάρτη.

Όπως θα δούμε όμως αργότερα στην ανάλυσή μας κανένα είδος δεν είναι απόλυτα ασυσχέτιστο με τα υπόλοιπα με αποτέλεσμα να υπάρχουν παράδοξες γειτνιάσεις οι οποίες μπορούν να εξηγηθούν με λίγη παραπάνω μελέτη των χαρακτηριστικών.

Αυξάνουμε ακόμα περισσότερο τις διαστάσεις του χάρτη σε 20 Χ 20 με σκοπό να αυξήσουμε τη λεπτομέρεια της απεικόνισης.
"""

som = joblib.load('som_20.pkl')
som_algorithm(som,25)

"""Παρατηρούμε πως στο χάρτη 20 Χ 20 ξεκινάνε να εμφανίζονται ξανά έντονες περιοχές ορίων και αυξάνεται πάλι ο αριθμός των περιοχών χαμηλής απόστασης στο χάρτη οι οποίες περικλείονται από όρια (κόκκινες περιοχές). Ακόμα και με K-Means με 25 clusters παρατηρούμε πως οπτικά τα clusters δε σέβονται απόλυτα τα όρια αυτών των περιοχών."""

total_cluster_comp(25)

som_algorithm(som,30)

"""Παρατηρούμε πως το πρόβλημα που υπάρχει με τα όρια δε λύνεται ακόμα και μετά την αύξηση του αριθμού των clusters του K-Means, αντιθέτως ξεκινά πλέον να παρτηρείται η διάσπαση των περιοχών χαμηλής απόστασης σε επιμέρους clusters, χωρίς λομως να υπάρχει κάποιο διακριτό όριο μεταξύ τους. """

total_cluster_comp(30)

"""Θα αυξήσουμε ακόμα περισσότερο τις διαστάσεις του χάρτη σε 25 Χ 25."""

som = joblib.load('som25.pkl')
som_algorithm(som,35)

"""Ο χάρτης 25 Χ 25 αποτελεί την καλύτερη λύση μέχρι στιγμής αφού όπως φαίνεται δημιουργεί εξίσου αρκετές περιοχές χαμηλής απόστασης και έντονα όρια, πράγματα που έλειπαν από τις προηγούμνες απόπειρές μας μέχρι στιγμής. 

Η επιλογή n_clusters = 35 για τον K-Means φαίνεται να ταιριάζει καλά στο συγκεκριμένο χάρτη, καθώς ο κάθε cluster φαίνεται να αντιστοιχεί σε μια διακριτή περιοχή, είτε αυτή είναι όριο, είτε περικλείεται από όρια. 
"""

total_cluster_comp(35)

"""Τέλος θα κάνουμε μία τελευταία προσπάθεια να αυξήσουμε πάλι τις διαστάσεις του χάρτη με σκοπό να ελέγξουμε τα όρια λεπτομέρειας στα οποία μπορούμε να φτάσουμε."""

som = joblib.load('som_30.pkl')
som_algorithm(som,35)

"""Παρατηρούμε πως στον πίνακα 30 Χ 30 απεικονίζονται όπως είναι λογικό περισσότερες περιοχές χαμηλής απόστασης και επίσης η ένταση των ορίων έχει μειωθεί αρκετά, καθώς οι μεταβάσεις είναι πιο ομαλές. Επίσης ο αριθμός των clusters του Κ-Means είναι και πάλι μικρός συγκριτικά με την τοπολογία του χάρτη για αυτό αυξάνουμε σε n_clusters = 40 και επαναλαμβάνουμε."""

som_algorithm(som, 40)

"""### Γενικά συμπεράσματα για σχέση διάστασης χαρτών - λεπτομέρειας και αριθμό Κ-Μeans clusters

Η αύξηση τψν διαστάσεων του χάρτη παρατηρήσαμε ότι εντείνει τη λεπτομέρεια αλλά και την τραχύτητα των περιοχών.

Αρχικά στον 10 Χ 10 το μεγαλύτερο μέρος του χάρτη ήταν περιοχές ορίων (κόκκινες), με ελάχιστα clusters να έχουν σχηματιστεί. Αυξάνοντας τις διαστάσεις παρατηρήσαμε σταδιακά την αύξηση τη δημιουργία όλο και περισσότερων μπλε περιοχών (χαμηλής απίστασης) οι οποίες αντιπροσωπευαν ομάδες με κοινά χαρακτηριστικά. Παράλληλα η ένταση των ορίων μειωνόταν, καθώς υπήρχε χώρος στο χάρτη για να απεικονιστούν ομαλά οι μεταβάσεις μεταξύ κατηγοριών, που στην περίπτωση των ταινιών δεν είναι τόσο απότομες (μίξεις ειδών). Τέλος, η υπερβολική αύξηση των διαστάσεων του χάρτη, όπως ήταν λογικό, επέφερε τα αντιθετα αποτελέσματα, με τα όρια σταδιακά να χάνονται και τον αριθμό των μπλε περιοχών να πολλαπλασιάζεται. Όσον αφορά το επιθυη=μητό επίπεδο λεπτομέρειας επιλέξαμε το χάρτη 25 Χ 25 ως μία ικανοποιητική λύση από πλευράς λεπτομέρειας και ικανότητας διάκρισης. 

Οι απαιτήσεις του K-Means ως προς τον αριθμό τον clusters που απαιτούνται για αντιπροσωπευτική απεικόνιση του χάρτη είναι άμεση συνάρτηση των διαστάσεών του.

Συγκεκριμένα παρατηρήσαμε πως με κάθε αύξηση της διάστασης του χάρτη έπρεπε να αυξάνουμε ανάλογα και τον αριθμό των clusters ώστε το clustering να ανταποκρίνεται στις νέες μπλε περιοχές που δημιουργούνταν. Αρκετές φορές βέβαια συναντήσαμε ένα πρόβλημα καθώς αντί τα clusters να προσαρμόζονται σε περιοχές που οπτικά φαίνοντας διακριτές, αυτά προχωρούσαν σε διάσπαση είτε των ορίων μεταξύ τους είτε σε εσωτερική διάσπαση ομαλών μπλε περιοχών. Πάντοτε όμως παρά τη δυσκολία κατανόησης της κατάτμησης αυτής από την οπτικοποίηση του χάρτη, όταν προχωρούσαμε στην αναλυτική παρουσίαση των χαρακτηριστικών των clusters το clustering που είχε γίνει "έβγαζε νόημα" καθώς τα νέα clusters αντιπροσώπευαν μια ιδιαίτερη υποκατηγορία. Το μόνο θέμα εδώ λοιπόν ήταν πως δεν δινόταν προτεραιότητα στη διασπαση κάποιον μεγάλων και ανιμοιόμορφων clusters, χωρίς κάποια αντιπροσωπευτική κατηγορία, πράγμα που αποδίδουμε σε εσωτερικές ίσως ομοιότητες των ταινιών στην περιγραφή τους, οι οποίες δεν απεικονίζονται σε επίπεδο κατηγοριών ταινιών.

### Ανάλυση τοπολογικών ιδιοτήτων με SOM

Στο βήμα αυτό θα χρησιμοποιήσουμε έναν **SOM** εκπαιδευμένο στο σύνολο του dataset, δηλαδή όλα τα 5000 δείγματα, όπου έχουμε χρησιμοποιήσει ως αλγόριθμο για το clustering τον **K-Means** με `n_clusters = 35`. Θεωρούμε πως στην συγκεκριμένη περίπτωση είχαμε ένα επιθυμητό επίπεδο λεπτομέρειας ως προς την τοπολογία και τις ιδιότητές της, το οποίο ερμηνεύουμε παρακάτω με της οπτικοποιήσεις που έχουμε κατασκευάσει.
"""

som = joblib.load('som25_full.pkl')
som_algorithm(som,35)

joblib.dump(som, 'som25_full_trained.pkl')

"""Από την πρώτη παραπάνω οπτικοποίηση του `U-Matrix` παρατηρούμε πως τα περισσότερα clusters διαχωρίζονται μεταξύ τους με όρια χαμηλής έντασης, πράγμα απόλυτα λογικό και επιθυμητό. 

Οι ταινίες δεν χωρίζονται σε αμοιβαίως αποκλειώμενες κατηγορίες αλλά αντιθέτως τα χαρακτηριστικά τους επιτρέπουν ομαλή μετάβαση μεταξύ κατηγριών, αλλά και μίξεις κάθε είδους, όπως θα παρατηρήσουμε σε λίγο.
"""

total_cluster_comp(35)

"""Στο παραπάνω κελί καλείται η συνάρτηση οπτικοποίησης `total_cluster_comp` με σκοπό να αναλύσουμε τις ιδιότητες κάθε cluster του SOM. Παρατηρούμε τα εξής:

Ως προς την αναλογία **Συχνότητα εμφάνισης εισόδου - Νευρώνες που Αντιστοιχούν στο cluster** : Παρατηρείται να τηρείται γενικά ο κανόνας ότι οι μεγαλύτερες σε συχνότητα είσοδοι απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Η σχέση όμως δεν είναι εντελώς αναλογική αφού η κατηγορία για `cluster_id = 9` με **678 δείγματα** στο χώρο εισόδου αναπαρίσταται με **31 νευρώνες** την ίδια στιγμή που η κατηγορία που αντιστοιχεί σε `cluster_id = 6` με **353 δείγματα** αναπαρίσταται με **29 νευρώνες**. Επίσης υπάρχουν και περιπτώσεις που δεν τηρείται καν η διάταξη στη σχέση εισόδου-νευρώνων, το οποίο πιθανόν οφείλεται στην ιδιαιτερότητα των δεδομένων.

Ως προς το περιεχόμενο των κυρίαρχων κατηγοριών κάθε cluster παρατηρούμε πως γενικά σα δομή η πρώτη κατηγορία συνήθως αποτελεί τη γενική κατηγορία του cluster και οι υπόλοιπες δείχνουν την κατεύθυνση προς την οποία κινείται το cluster. Γι' αυτό παρατηρείται συχνά η ύπραξη κοινής πρώτης κατηγορίας μεταξύ διαφορετικών clusters. Οι υπόλοιπες κατηγορίες συνήθως διαφέρουν αρκετά και άρα οδηγούν στη διάκριση των clusters.

Θεωρούμε πως πολύ σημαντικά clusters για μελέτη είναι τα παρακάτω.

Το cluster 9 με 678 samples τα οποία όλα ανήκουν στην κατηγορία "Drama". Δεδομένης της αντοχής αυτού του cluster στο sub-clustering μπορούμε να προσδιορίσουμε ίσως μέσα από μελέτη των χαρακτηριστικών των ταινιών που ανήκουν σε αυτό κάποια πολύ έντονα χαρακτηριστικά των ταινιών "Drama" τα οποία προκαλούν τη διάκρισή τους από τα άλλα είδη.

Ιδιαίτερη περίπτωση αποτελεί το cluster με `"cluster_id = 8"`, το οποίο αν και περιέχει 321 δείγματα, δεν έχει καμία κατηγορία με πάνω από 50 δείγματα. Παρακάτω παρουσιάζονται όλες οι κατηγορίες με τη συχνότητα εμφάνισής τους.
"""

[('"Japanese Movies"', 52), ('"Science Fiction"', 42), ('"Silent film"', 42), ('"Adventure"', 26), ('"Chinese Movies"', 22), ('"Crime Fiction"', 22), ('"World cinema"', 22), ('"Musical"', 21), ('"War film"', 20), ('"Action/Adventure"', 15), ('"Mystery"', 15), ('"Western"', 15), ('"Family Film"', 14), ('"Fantasy"', 14), ('"Bollywood"', 12), ('"Martial Arts Film"', 12), ('"Children\\s/Family"', 11), ('"Comedy-drama"', 9), ('"Cult"', 7), ('"Anime"', 6), ('"Television movie"', 6), ('"Historical drama"', 5), ('"Costume drama"', 4), ('"Film noir"', 4), ('"Monster"', 4), ('"Art film"', 3), ('"Black comedy"', 3), ('"Children\\s Fantasy"', 3), ('"Children\\s"', 3), ('"Disaster"', 3), ('"Family-Oriented Adventure"', 3), ('"Filipino Movies"', 3), ('"Historical fiction"', 3), ('"Indie"', 3), ('"Suspense"', 3), ('"Time travel"', 3), ('"Action"', 2), ('"Animated cartoon"', 2), ('"Biographical film"', 2), ('"Crime Drama"', 2), ('"Experimental film"', 2), ('"Family Drama"', 2), ('"Horror"', 2), ('"Music"', 2), ('"Superhero movie"', 2), ('"Thriller"', 2), ('"Alien Film"', 1), ('"Apocalyptic and post-apocalyptic fiction"', 1), ('"Avant-garde"', 1), ('"Biopic feature"', 1), ('"Black-and-white"', 1), ('"Buddy film"', 1), ('"Children\\s Entertainment"', 1), ('"Christian film"', 1), ('"Christmas movie"', 1), ('"Comedy"', 1), ('"Crime Thriller"', 1), ('"Culture & Society"', 1), ('"Detective fiction"', 1), ('"Detective"', 1), ('"Documentary"', 1), ('"Dogme 95"', 1), ('"Epic"', 1), ('"Fantasy Adventure"', 1), ('"Heaven-Can-Wait Fantasies"', 1), ('"Historical Epic"', 1), ('"History"', 1), ('"Kitchen sink realism"', 1), ('"Language & Literature"', 1), ('"Melodrama"', 1), ('"Monster movie"', 1), ('"Musical Drama"', 1), ('"Musical comedy"', 1), ('"Operetta"', 1), ('"Pre-Code"', 1), ('"Prison"', 1), ('"Psychological thriller"', 1), ('"Punk rock"', 1), ('"Religious Film"', 1), ('"Romantic comedy"', 1), ('"Satire"', 1), ('"Slasher"', 1), ('"Sports"', 1), ('"Steampunk"', 1), ('"Superhero"', 1), ('"Surrealism"', 1), ('"Tamil cinema"', 1), ('"Teen"', 1), ('"Wuxia"', 1)]

"""Αν και αρχικά δε βγάζει νόημα η δημιουργία ενός τόσο μεγάλου cluster χωρίς ιδαίτερη συνλαφεια μεταξύ των κατηγοριών, μπορούμε να παρατηρήσουμε πως οι κυριότερες απευθύνονται στα "αγαπημένα" είδη διάφορων λαών. Βλέπουμε δηλαδή, `"Chinese Movies", "Japanese Movies", "World Cinema", "Western", "Bollywood"` κλπ.
Αυτός είναι ίσως και ο λόγος δημιουργίας του cluster από τον αλγόριθμο. Εδώ είναι σημαντικό να αναφέρουμε πως ακόμα και όταν αυξάναμε τον αριθμό των clusters το συγκεκριμένο cluster δεν έδειχνε κάποια τάση να διασπαστεί σε επιμέρους clusters με πιο συγκεκριμένο περιεχόμενο. Το συγκεκριμένο cluster λοιπόν έχει ιδιαίτερη αξία για περαιτέρω έρευνα, καθώς αναλύοντας το περιεχόμενο των ταινιών του μπορούμε να προσδιορίσουμε ομοιότητες που κανουν τα "εθνικά" είδη περισσότερα "παγκόσμια" από όσο νομίζουμε.

Παρακάτω απεικονίζουμε τα clusters σε μορφή γράφου γειτνίασης για εξαγωγή επιιπλέον συμπερασμάτων.
"""

test = total_cluster_comp(35)
top3 = test['Top 3 Categories'].to_list()
top3final = []
for i in range(len(top3)):
    top3final.append(top3[i][0][0]+','+top3[i][1][0]+','+top3[i][2][0])
top3 = top3final
print(top3)

plot_topology0(35)

plot_topology(35,top3)

"""**ΣΕ ΠΕΡΙΠΤΩΣΗ ΠΟΥ ΔΕΝ ΕΜΦΑΝΙΖΕΤΑΙ Ο ΔΙΑΔΡΑΣΤΙΚΟΣ ΓΡΑΦΟΣ ΠΟΥ ΒΡΙΣΚΕΤΑΙ ΣΤΟ ΑΚΡΙΒΩΣ ΠΑΡΑΠΑΝΩ ΚΕΛΙ, ΕΚΤΕΛΕΣΤΕ ΤΟ ΠΑΡΑΚΑΤΩ ΚΕΛΙ ΓΙΑ ΝΑ ΤΟΝ ΕΙΣΑΓΕΤΕ ΜΕ HTML**

Το παρακάτω κελί δουλεύει επιτυχώς σε `Jupyter`, όχι όμως σε `Colab`
"""

from IPython.display import IFrame
IFrame(src='./plot_25_35.html', width=1200, height=600)

"""Από το γράφο μπορούμε να παρατηρήσουμε τις "γειτονιές" του χάρτη. 

Χαρακτηριστικά παραδείγματα με τα οποία θα ασχοληθούμε είναι το `cluster = 6` με κύριες κατηγορίες τις `["Horror","Thriller","Science Fiction"]`
και το `cluster = 11` με κύριες κατηγορίες τις κατηγορίες `["Family Film","Comedy","Childrens/Family"]`. Τα δύο αυτά, εντελώς ομολογουμένων αντίθετα, clusters χωρίζουν σε οποιδήποτε μονοπάτι στο γράφο τουλάχιστον δύο κόμβοι. Αντίστοιχα αποτελέσματα μπορούν να εξαχθούν και από το `U-matrix`. 

Επιπλέον, μια γενική κατηγορία, συνήθως γειτονέυει με τις υποκατηγορίες της. Συγκεκριμένα, το `cluster = 9` που αναπαριστά τη γενική κατηγορία `"Drama"` συνορεύει με επιμέρους κατηγορές δραμάτων που αντιπροσωπεύονται από τους clusters : `[16, 19, 1, 34, 24]`

Παραδόξως βέβαια γειτονικές είναι και οι κατηγορίες 28 και 10 που αντιπροσωπεύουν θρίλερ μυστηρίου και ρομαντικές κομεντί αντίστοιχα. Αν και δεν φαίνεται να υπάρχει κάποια προφανής σύνδεση μεταξύ των κατηγοριών, εικάζουμε πως η σύνδεση αυτή οφείλεται σε ομοιότητες ως προς το περιεχόμενο και κυρίως ως προς το background τοο οποίο περιγράφεται κατά τη σύνοψη. 

Ακολούθως οι `clusters 2 και 27` συνορεύουν ενώ οι κατηγορίες τους φαίνονται αρχικά εντελώς διαφορετικές. Ο 2 αντιπροσωπεύει `["Thriller","Drama","Psychological Thriller"]` και ο 27 `["Comedy","Parody","Indie"]` κατηγορίες τρομερά διαφορετικές. Παρ' όλα αυτά γνωρίζοντας την ύπαρξη ταινιών που σατιρίζουν τα blockbuster ανεξαρτήτων κατηγορίας, [π.χ. "Scary Movies" σατιρίζει "Scream"] , θεωρούμε πως οι παροδίες αποτελούν "πασπαρτού" που γεφυρώνει τις κατά τα άλλα άσχετες αυτές κατηγορίες.

# Τελική παράδοση άσκησης

- Θα παραδώσετε στο eclass το παρόν notebook επεξεργασμένο ή ένα νέο με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. 
- Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στον τελικό χάρτη με τα κελιά ορατά που θα παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. 
- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**.
- Στο **zip** που θα παραδώσετε πρέπει να βρίσκονται **2 αρχεία (το .ipynb και το .py του notebook σας)**.

<table>
  <tr><td align="center">
    <font size="4">Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα!</font>
</td>
  </tr>
</table>
"""