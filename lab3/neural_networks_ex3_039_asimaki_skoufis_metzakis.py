# -*- coding: utf-8 -*-
"""Neural_Networks_Ex3_039_Asimaki_Skoufis_Metzakis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hSd0MuVyMlKt8818IMAnfnQepemPFmAa

# Βαθιά μάθηση στο CIFAR-100

Ομάδα: 39  
Ασημάκη Γεωργία-Γρηγορία: 03116197, a.tzotzo@gmail.com  
Μετζάκης Ιωάννης: 03116202, johnmetzakis@gmail.com   
Σκούφης Πέτρος: 03116141, pskoufis13@gmail.com

## Εισαγωγή και επισκόπηση του συνόλου δεδομένων
"""

from __future__ import absolute_import, division, print_function, unicode_literals # legacy compatibility

import tensorflow as tf
from tensorflow.keras import datasets, layers, models, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time

# helper functions

# select from from_list elements with index in index_list
def select_from_list(from_list, index_list):
  filtered_list= [from_list[i] for i in index_list]
  return(filtered_list)

# append in filtered_list the index of each element of unfilterd_list if it exists in in target_list
def get_ds_index(unfiliterd_list, target_list):
  index = 0
  filtered_list=[]
  for i_ in unfiliterd_list:
    if i_[0] in target_list:
      filtered_list.append(index)
    index += 1
  return(filtered_list)

# select a url for a unique subset of CIFAR-100 with 20, 40, 60, or 80 classes
def select_classes_number(classes_number = 20):
  cifar100_20_classes_url = "https://pastebin.com/raw/nzE1n98V"
  cifar100_40_classes_url = "https://pastebin.com/raw/zGX4mCNP"
  cifar100_60_classes_url = "https://pastebin.com/raw/nsDTd3Qn"
  cifar100_80_classes_url = "https://pastebin.com/raw/SNbXz700"
  if classes_number == 20:
    return cifar100_20_classes_url
  elif classes_number == 40:
    return cifar100_40_classes_url
  elif classes_number == 60:
    return cifar100_60_classes_url
  elif classes_number == 80:
    return cifar100_80_classes_url
  else:
    return -1

# load the entire dataset
(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.cifar100.load_data(label_mode='fine')

print(x_train_all.shape)

"""Η κάθε ομάδα θα δουλέψει με ένα μοναδικό ξεχωριστό υποσύνολο του CIFAR-100
Στο επόμενο κελί, αντικαταστήστε την τιμή της μεταβλητής `team_seed` με τον αριθμό της ομάδας σας.
"""

# REPLACE WITH YOUR TEAM NUMBER
team_seed = 39

"""Στο επόμενο κελί μπορείτε να διαλέξετε το πλήθος των κατηγορίων σας: 20 (default), 40, 60 ή 80."""

NUM_OF_CLASSES = 20

# select the number of classes
cifar100_classes_url = select_classes_number(NUM_OF_CLASSES)

"""Δημιουργούμε το μοναδικό dataset της ομάδας μας:"""

team_classes = pd.read_csv(cifar100_classes_url, sep=',', header=None)
CIFAR100_LABELS_LIST = pd.read_csv('https://pastebin.com/raw/qgDaNggt', sep=',', header=None).astype(str).values.tolist()[0]

our_index = team_classes.iloc[team_seed,:].values.tolist()
our_classes = select_from_list(CIFAR100_LABELS_LIST, our_index)
train_index = get_ds_index(y_train_all, our_index)
test_index = get_ds_index(y_test_all, our_index)

x_train_ds = np.asarray(select_from_list(x_train_all, train_index))
y_train_ds = np.asarray(select_from_list(y_train_all, train_index))
x_test_ds = np.asarray(select_from_list(x_test_all, test_index))
y_test_ds = np.asarray(select_from_list(y_test_all, test_index))

# print our classes
print(our_classes)

our_dict = {our_index[i] : i for i in range(len(our_index))}

print(x_train_ds[1].shape)

# get (train) dataset dimensions
data_size, img_rows, img_cols, img_channels = x_train_ds.shape

# set validation set percentage (wrt the training set size)
validation_percentage = 0.15
val_size = round(validation_percentage * data_size)

# Reserve val_size samples for validation and normalize all values
x_val = x_train_ds[-val_size:]/255
y_val = y_train_ds[-val_size:]
x_train = x_train_ds[:-val_size]/255
y_train = y_train_ds[:-val_size]
x_test = x_test_ds/255
y_test = y_test_ds

print(len(x_val))

# summarize loaded dataset
print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))
print('Validation: X=%s, y=%s' % (x_val.shape, y_val.shape))
print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))

# get class label from class index
def class_label_from_index(fine_category):
  return(CIFAR100_LABELS_LIST[fine_category.item(0)])

# plot first few images
plt.figure(figsize=(6, 6))
for i in range(9):
	# define subplot
  plt.subplot(330 + 1 + i).set_title(class_label_from_index(y_train[i]))
	# plot raw pixel data
  plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))
  #show the figure
plt.show()

"""Γραφικές παραστάσεις εκπαίδευσης και απόδοση στο σύνολο ελέγχου"""

# plot diagnostic learning curves
def summarize_diagnostics(history):
	plt.figure(figsize=(8, 8))
	plt.suptitle('Training Curves')
	# plot loss
	plt.subplot(211)
	plt.title('Cross Entropy Loss')
	plt.plot(history.history['loss'], color='blue', label='train')
	plt.plot(history.history['val_loss'], color='orange', label='val')
	plt.legend(loc='upper right')
	# plot accuracy
	plt.subplot(212)
	plt.title('Classification Accuracy')
	plt.plot(history.history['accuracy'], color='blue', label='train')
	plt.plot(history.history['val_accuracy'], color='orange', label='val')
	plt.legend(loc='lower right')
	return plt
 
# print test set evaluation metrics
def model_evaluation(model, evaluation_steps):
	print('\nTest set evaluation metrics')
	loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
	print("loss: {:.2f}".format(loss0))
	print("accuracy: {:.2f}".format(accuracy0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model, evaluation_steps)

"""## Δοκιμές διαφορετικών μοντέλων

Μπορείτε είτε να δοκιμάσετε μοντέλα "from scratch", όπου ορίζετε την αρχιτεκτονική του δικτύου όπως θέλετε, είτε να χρησιμοποιήσετε μεταφορά μάθησης.

## Μοντέλα "from scratch"

Μπορείτε να τροποποιήσετε/αλλάξετε το αρχικό μικρό συνελικτικό δίκτυο του παραδείγματος. Μπορείτε να συμβουλευτείτε 
- τη [βιβλιογραφία απο το leaderboard του CIFAR-100](https://benchmarks.ai/cifar-100) για αρχιτεκτονικές και παραμέτρους των δικτύων
- ή/και να πάρετε ιδέες [από σχετική αναζήτηση στο Google Scholar](https://scholar.google.gr/scholar?hl=en&as_sdt=0%2C5&q=cifar+100+cnn&oq=cifa)

Παρακάτω θα κάνουμε μια απόπειρα να επεκτείνουμε το απλό CNN μοντέλο που μας δόθηκε σε ένα πιο βαθύ μοντέλο εισάγοντας επίσης νέες μορφές επιπέδων, όπως τα επίπεδα **BatchNormalization** και **Dropout**. Παρακάτω εξηγούμε συνοπτικά την σημασία αυτών των επιπέδων στο δίκτυό μας.

* Batch Normalization: 
Το επίπεδο αυτό κανονικοποιεί τις εισόδους κάθε επιπέδου με αποτέλεσμα να βελτιώνει τη σωστή διάδοση των σφαλμάτων προς τα πίσω στο δίκτυο και μειώνοντας την ταχύτητα εκπαίδευσης.

https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/

* Dropout:
Το επίπεδο αυτό χρησιμοποιείται με σκοπό να καταπολεμήσει το overfitting, αφού τυχαία αγνοεί ένα ποσοστό από τις εξόδους του τρέχοντος επιπέδου στο επόμενο επίπεδο.

https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/
"""

# simple function to map labels to our labels

def map_label(x):
    x = [[our_dict[i[0]]] for i in x]
    return np.array(x)

# we user prefetch https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch 
# see also AUTOTUNE
# the dataset is now "infinite"

BATCH_SIZE = 128
AUTOTUNE = tf.data.experimental.AUTOTUNE # https://www.tensorflow.org/guide/data_performance

def _input_fn(x,y, BATCH_SIZE):
    ds = tf.data.Dataset.from_tensor_slices((x,y))
    ds = ds.shuffle(buffer_size=data_size)
    ds = ds.repeat()
    if (augmentation == True):
        augs = [crop_aug, flip_aug, color_aug]
        for f in augs:
            ds = ds.map(f)
    ds = ds.batch(BATCH_SIZE)
    ds = ds.prefetch(buffer_size=AUTOTUNE)
    #ds = ds.cache()
    return ds

augmentation = False
train_ds =_input_fn(x_train,map_label(y_train), BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,map_label(y_val), BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,map_label(y_test), BATCH_SIZE) #PrefetchDataset object
# steps_per_epoch and validation_steps for training and validation: https://www.tensorflow.org/guide/keras/train_and_evaluate

def train_model(model, epochs = 10, steps_per_epoch = 2, validation_steps = 1, cb = False):
    if cb == True:
        callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights = True)
        history = model.fit(train_ds, callbacks=[callback], epochs=epochs, 
                        steps_per_epoch=steps_per_epoch, validation_data=validation_ds, 
                     validation_steps=validation_steps)
    else:
        history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps)
    return(history)

"""Στο σημείο αυτό παρουσιάζουμε το τελικό μοντέλο στο οποίο καταλήξαμε, έπειτα από σταδιακή προσθήκη επιπέδων. Όπως φαίνεται στο δίκτυό μας επαναλμβάνεται μια δομική μονάδα από δύο συνέλιξεις - BatchNormalization - MaxPooling2D - Dropout, η οποία επαναλαμβάνεται 5 φορές με αύξηση των παραμέτρων όσο προψωράμε πιο βαθιά, αφού κερδίζουμε υπολογιστικό χρόνο από τον υποδιπλασιασμό της διάστασης της εισόδου. 

Στο τέλος του δικτύου γίνεται Flatten και με διαδοχικά Πυκνά επίπεδα καταλήγουμε στην έξοδο.
"""

L2_DECAY_RATE = 0.0005

def init_custom_model(summary):    
    model = models.Sequential()
    
    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(32,32,3)))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(64, kernel_size=(3, 3),activation='elu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))

    model.add(layers.Conv2D(64, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(128, kernel_size=(2, 2),activation='elu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))

    model.add(layers.Conv2D(128, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(128, kernel_size=(3, 3),  padding='same', activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(256, kernel_size=(2, 2), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))
    
    model.add(layers.Conv2D(256, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(512, kernel_size=(2, 2), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))
    
    model.add(layers.Conv2D(512, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(1024, kernel_size=(2, 2), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.25))
    
    model.add(layers.GlobalAveragePooling2D())
    
    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.25))
    #model.add(layers.Dense(20, activation='softmax'))
    #model.add(layers.Dropout(0.25))
    model.add(layers.Dense(128, activation='relu'))
    #model.add(layers.Dense(64, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.25))
    model.add(layers.Dense(80, activation='softmax'))
    
    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
    if summary: 
        model.summary()
    return model

"""**Αποτελέσματα για 20 κλάσεις**

Εκπαίδευση με EarlyStopping με μεγάλο patience (30 εποχές)
"""

CUSTOM_MODEL_20 = init_custom_model(summary = True)
CUSTOM_MODEL_20_history = train_model(CUSTOM_MODEL_20, 400, 66, 5, cb=True)

CUSTOM_MODEL_20_loss, CUSTOM_MODEL_20_acc = CUSTOM_MODEL_20.evaluate(x_test, map_label(y_test))

"""Το μοντέλο μας τελικά σταματά σε 154 εποχές."""

CUSTOM_MODEL_20 = init_custom_model(summary = True)
CUSTOM_MODEL_20_history = train_model(CUSTOM_MODEL_20, 100, 66, 5, cb=False)

CUSTOM_MODEL_20_loss, CUSTOM_MODEL_20_acc = CUSTOM_MODEL_20.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_20_history)

"""Απενεργοποιώντας τα callbacks για EarlyStopping, παρατηρούμε πως το μοντέλο παρουσιάζει τεράστιο overfitting, παρά τις προσπάθειές μας να αντιμετωπίσουμε κάτι τέτοια εισάγοντας τα κατάλληλα επίπεδα κατά την κατασκευή του. Επιπλέον, από την 20η περίπου εποχή και έπειτα τα αποτελέσματα στο validation set όσον αφορά loss και accuracy φαίνεται να έχουν παγιωθεί και να ταλαντώνονται χωρίς ουσιαστική βελτίωση.

**Αποτελέσματα για 40 κλάσεις**
"""

CUSTOM_MODEL_40 = init_custom_model(summary = True)
CUSTOM_MODEL_40_history = train_model(CUSTOM_MODEL_40, 200, 66, 5, False)

CUSTOM_MODEL_40_loss, CUSTOM_MODEL_40_acc = CUSTOM_MODEL_40.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_40_history)

"""Όπως φαίνεται από τις γραφικές το δίκτυο και για τις 40 κλάσεις παρουσιάζει αρκετό overfitting, χωρίς ουσιαστικές αλλαγές στα αποτελέσματα στο validation set μετά το πέρας της εποχής 50. Το **παράδοξο** της υπόθεσης είναι πως για το πρόβλημα των 40 κλάσεων το οποίο είναι πιο δύσκολο από αυτό των 20 καταλήγουμε σε καλύτερα αποτελέσμαρα όσον αφορά το accuracy και το loss.

Και εδώ παρουσιάζονται τα αποτελέσματα με χρήση EarlyStopping με patience 30 εποχές.
"""

CUSTOM_MODEL_40 = init_custom_model(summary = True)
CUSTOM_MODEL_40_history = train_model(CUSTOM_MODEL_40, 400, 66, 5, True)

CUSTOM_MODEL_40_loss, CUSTOM_MODEL_40_acc = CUSTOM_MODEL_40.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_40_history)

"""H παραδόξως καλή συμπεριφορά του δικτύου συνεχίζεται και με EarlyStopping, όπου σταματάει σε μικρότερο αριθμό εποχών συγκριτικά με το πρόβλημα των 20 κλάσεων (146 από 154) πετυχαίνοντας μεγαλύτερο accuracy στο test set (65.75%). Παρ'όλα αυτά το overfitting είναι αισθητό και θα δούμε κατά πόσο μπορούμε να το καταπολεμήσουμε με Data Augmentation.

**Αποτελέσματα για 60 κλάσεις**
"""

CUSTOM_MODEL_60 = init_custom_model(summary = True)
CUSTOM_MODEL_60_history = train_model(CUSTOM_MODEL_60, 200, 66, 5, False)

CUSTOM_MODEL_60_loss, CUSTOM_MODEL_60_acc = CUSTOM_MODEL_60.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_60_history)

"""Παρατηρούμε πως το πρόβλημα ακολουθεί την ίδια λογική με τα προηγούμενα, όντας όμως λίγο πιο δύσκολο. Εμφανίζεται λοιπόν υπερεκπαίδευση η οποία μετά την 75η εποχή ξεκινά και γίνεται αισθητή.

Θα κάνουμε τώρα χρήση EarlyStopping.
"""

CUSTOM_MODEL_60 = init_custom_model(summary = True)
CUSTOM_MODEL_60_history = train_model(CUSTOM_MODEL_60, 400, 66, 5, True)

CUSTOM_MODEL_60_loss, CUSTOM_MODEL_60_acc = CUSTOM_MODEL_60.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_60_history)

"""Παρατηρούμε πως και σε αυτό το πρόβλημα πετυχαίνουμε accuracy της τάξης του 60% στο test set. Επίσης η ένταση του overfitting μειώνεται με την αύξηση της δυσκολίας του προβλήματος, εξακολουθεί όμως να είναι μεγάλη (80% accuracy in training - 60% in testing).

**Αποτελέσματα για 80 κλάσεις**
"""

CUSTOM_MODEL_80 = init_custom_model(summary = True)
CUSTOM_MODEL_80_history = train_model(CUSTOM_MODEL_80, 250, 66, 5, False)

CUSTOM_MODEL_80_loss, CUSTOM_MODEL_80_acc = CUSTOM_MODEL_80.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_80_history)

"""Παρατηρούμε πως στο πρόβλημα των 80 κλάσεων έχει μειωθεί αρκετά το overfitting, παρουσιάζεται βέβαια ακόμα. Επιπλέον παρά τη δυσκολία του πετυχαίνουμε 56% accuracy στο test set το οποίο είναι ικανοποιητικό."""

CUSTOM_MODEL_80 = init_custom_model(summary = True)
CUSTOM_MODEL_80_history = train_model(CUSTOM_MODEL_80, 400, 66, 5, True)

CUSTOM_MODEL_80_loss, CUSTOM_MODEL_80_acc = CUSTOM_MODEL_80.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_80_history)

"""Επιπλέον, κάνοντας χρήση EarlyStopping παρατηρούμε πως γα τις 80 κλάσεις έχει αυξηθεί αρκετά ο αριθμός των εποχών που απαιτούνται πριν γίνει διακοπή σε 242 εποχές.

#### Adding Data Augmentation

Παρακάτω παρουσιάζονται διάφορες συστοιχίες από προεπεξεργασία που δοκιμάσαμε για επάυξηση δεδομένων. Επιλέξαμε να εισάγουμε τα επίπεδα αυτά στο δίκτυό μας με σκοπό να επωφεληθούμε από τη χρήση της GPU, όπως αναφερόταν και στο documentation του tensorflow.
"""

IMG_SHAPE = x_train_ds[1].shape

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(10),
  tf.keras.layers.experimental.preprocessing.RandomZoom(.2, .2)
])


data_augmentation1 = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomRotation(1)])
    
data_augmentation2 = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomContrast(0.6),
  tf.keras.layers.experimental.preprocessing.RandomRotation(5)])

data_augmentation3 = tf.keras.Sequential([
  layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
  layers.experimental.preprocessing.RandomRotation(0.1),
])

data_augmentation4 = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation(0.1),  
    layers.experimental.preprocessing.RandomZoom(.2, .2),
    layers.experimental.preprocessing.RandomContrast(0.2),
])

L2_DECAY_RATE = 0.0005

def init_custom_aug_model(summary):    
    model = models.Sequential()
    
    model.add(data_augmentation3)
    
    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(32,32,3)))
    model.add(layers.BatchNormalization())
    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(64, kernel_size=(3, 3),activation='elu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.5))

    model.add(layers.Conv2D(64, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(128, kernel_size=(2, 2),activation='elu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.5))

    model.add(layers.Conv2D(128, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(128, kernel_size=(3, 3),  padding='same', activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(256, kernel_size=(2, 2), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.5))
    
    model.add(layers.Conv2D(256, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(256, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(512, kernel_size=(2, 2), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.5))
    
    model.add(layers.Conv2D(512, kernel_size=(3, 3),  padding='same',activation='relu',kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.Conv2D(512, kernel_size=(3, 3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    #model.add(layers.Conv2D(1024, kernel_size=(2, 2), padding='same', activation='relu', kernel_regularizer=regularizers.l2(L2_DECAY_RATE)))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(layers.Dropout(0.5))
    
    model.add(layers.GlobalAveragePooling2D())
    
    model.add(layers.Flatten())
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.5))
    #model.add(layers.Dense(20, activation='softmax'))
    #model.add(layers.Dropout(0.25))
    model.add(layers.Dense(128, activation='relu'))
    #model.add(layers.Dense(64, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(80, activation='softmax'))
    
    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])
    if summary: 
        model.summary()
    return model

CUSTOM_MODEL_AUG = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_history = train_model(CUSTOM_MODEL_AUG, 500, 66, 5)

CUSTOM_MODEL_AUG_loss, CUSTOM_MODEL_AUG_acc = CUSTOM_MODEL_AUG.evaluate(x_test, map_label(y_test))

CUSTOM_MODEL_AUG = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_history = train_model(CUSTOM_MODEL_AUG, 250, 66, 5)

CUSTOM_MODEL_AUG_loss, CUSTOM_MODEL_AUG_acc = CUSTOM_MODEL_AUG.evaluate(x_test, map_label(y_test))

CUSTOM_MODEL_AUG = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_history = train_model(CUSTOM_MODEL_AUG, 400, 66, 5)

CUSTOM_MODEL_AUG_loss, CUSTOM_MODEL_AUG_acc = CUSTOM_MODEL_AUG.evaluate(x_test, map_label(y_test))

CUSTOM_MODEL_AUG = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_history = train_model(CUSTOM_MODEL_AUG, 400, 66, 5)

CUSTOM_MODEL_AUG_loss, CUSTOM_MODEL_AUG_acc = CUSTOM_MODEL_AUG.evaluate(x_test, map_label(y_test))

CUSTOM_MODEL_AUG = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_history = train_model(CUSTOM_MODEL_AUG, 400, 66, 5)

CUSTOM_MODEL_AUG_loss, CUSTOM_MODEL_AUG_acc = CUSTOM_MODEL_AUG.evaluate(x_test, map_label(y_test))

"""**Αποτελέσματα για 20 κλάσεις**"""

CUSTOM_MODEL_AUG_20 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_20_history = train_model(CUSTOM_MODEL_AUG_20, 200, 66, 5, cb = False)

CUSTOM_MODEL_AUG_20 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_20_history = train_model(CUSTOM_MODEL_AUG_20, 400, 66, 5, cb = True)

CUSTOM_MODEL_AUG_20_loss, CUSTOM_MODEL_AUG_20_acc = CUSTOM_MODEL_AUG_20.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_AUG_20_history)

"""Παρατηρούμε πως με τη χρήση του data augmentation στο παράδειγμα με τις 20 κλάσεις καταφέρνουμε να εξαλείψουμε το overfitting, ενώ παράλληλα επιβραδύνθηκε και η διαδικασία της εκπαίδευσης, όπως αυτή αποτυπώνεται μέσα από τις εποχές που χρειάστηκαν έως το EarlyStopping σταματήσει την εκπαίδευση. Συγκεκριμένα χρειαστήκαμε 250 εποχές συγκριτικά με τις 154 που χρειάστηκαν χωρίς το data augmentation. Δυστυχώς, η χρήση data augmentation συνοδεύτηκε από μια μικρή μείωση στο accuracy στο test set από 63% σε 61%.

**Αποτελέσματα για 40 κλάσεις**
"""

CUSTOM_MODEL_AUG_40 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_40_history = train_model(CUSTOM_MODEL_AUG_40, 250, 66, 5, cb = False)

CUSTOM_MODEL_AUG_40_loss, CUSTOM_MODEL_AUG_40_acc = CUSTOM_MODEL_AUG_40.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_AUG_40_history)

CUSTOM_MODEL_AUG_40 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_40_history = train_model(CUSTOM_MODEL_AUG_40, 400, 66, 5, cb = True)

CUSTOM_MODEL_AUG_40_loss, CUSTOM_MODEL_AUG_40_acc = CUSTOM_MODEL_AUG_40.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_AUG_40_history)

"""Παρατηρούμε πως όπως και πριν με το Data Augmentation εξαλείφεται εντελώς το overfitting. Παρ'όλα αυτά σε αυτό το πρόβλημα έχουμε αισθητή πτώση του accuracy κατά 10% το οποίο δεν είναι σε καμία περίπτωση επιθμητό. Αυτό που εικάζουμε είναι πως ο μετασχηματισμός αυτός δυσκιολεύει υπερβολικά το πρόβλημα της κατηγοριοποίησης εικόνων σε 40 κατηγορίες, αλλά όποια από τις επαυξήσεις δεδομένων και αν χρησιμοποιήσαμε παίρναμε το ίδιο αποτέλεσμα.

**Αποτελέσματα για 60 κλάσεις**
"""

CUSTOM_MODEL_AUG_60 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_60_history = train_model(CUSTOM_MODEL_AUG_60, 250, 66, 5, cb = False)

CUSTOM_MODEL_AUG_60_loss, CUSTOM_MODEL_AUG_60_acc = CUSTOM_MODEL_AUG_60.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_AUG_60_history)

CUSTOM_MODEL_AUG_60 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_60_history = train_model(CUSTOM_MODEL_AUG_60, 400, 66, 5, cb = True)

CUSTOM_MODEL_AUG_60_loss, CUSTOM_MODEL_AUG_60_acc = CUSTOM_MODEL_AUG_60.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_AUG_60_history)

"""**Αποτελέσματα για 80 κλάσεις**"""

CUSTOM_MODEL_AUG_80 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_80_history = train_model(CUSTOM_MODEL_AUG_80, 250, 66, 5, cb = False)

CUSTOM_MODEL_AUG_80_loss, CUSTOM_MODEL_AUG_80_acc = CUSTOM_MODEL_AUG_80.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_AUG_80_history)

CUSTOM_MODEL_AUG_80 = init_custom_aug_model(summary = False)
CUSTOM_MODEL_AUG_80_history = train_model(CUSTOM_MODEL_AUG_80, 400, 66, 5, cb = True)

CUSTOM_MODEL_AUG_80_loss, CUSTOM_MODEL_AUG_80_acc = CUSTOM_MODEL_AUG_80.evaluate(x_test, map_label(y_test))

summarize_diagnostics(CUSTOM_MODEL_AUG_80_history)

"""Όπως και στα προηγούμενα προβλήματα έτσι και σε αυτό το data augmentation εξάλειψε εντελ΄ςω το overfitting. Στη συγκεκριμένη περίπτωση όμω το ήδη δύσκολο πρόβλημα της κατηγοριοποίησης σε 80 κατηγορίες έγινε ακόμα χειρότερο με αποτέλεσμα να μειωθεί πολύ το τελικό accuracy. Συγκεκριμένα παρατηρούμε μια μείωση της τάξης του 23% στο test set πράγμα μη αποδεκτό.

#### Γενικές Παρατηρήσεις σχετικά με Data Augmentation

Μετά την εφαρμογή του Data Augmentation στα δεδομένα μας παρατηρήσαμε τα παρακάτω.

Αρχικά, θέλουμε να σημειώσουμε πως το δυσκολότερο κομμάτι όπως εμείς συμπεράναμε στη διαδικασία του Data Augmentation είναι η εύρεση ενός μετασχηματισμού ο οποίος αποτελεί τη μέση λύση μεταξύ ενός πολύ απλού μετασχηματισμού που δεν βελτιώνει καθόλου στην ουσία τα δεδομένα του προβλήματος και ενός αρκετά σύνθετου, ο οποίος αυξάνει δυσανάλογα τη δυσκολία του προβλήματος συγκριτικά με τα επιπλέον δεδομένα που προσφέρει. Αρκετός χρόνος από τις δοκιμές μας "σπαταλήθηκε" άσκοπα προσπαθώντας να κάνουμε τα μοντέλα μας να κερδίσουν από πολύ σύνθετους μετασχηματισμούς πράγμα που κατανοήσαμε στην πορεία πως είναι λάθος.

Όσον αφορά τη διαφορές μεταξύ της εκπαίδευσης με και χωρίς data augmentation, όπως είναι και λογικό συμπέρασμα των όσων αναφέραμε νωρίτερα η εκπαίδευση με data augmentation ως πιο δύσκολο πρόβλημα απαιτούσε περισσότερο χρόνο (όπως αυτός αποτυπώνεται από τις εποχές εκπαίδευσης). Επιπλέον, παρατηρήσαμε πως επιτυγχάναμε σχεδόν ολική εξάλειψη του overfitting. 

Παρ'όλα αυτά δεν καταφέραμε να πετύχουμε καλύτερα αποτελέσματα όσον αφορά το accuracy με τη χρήση data augmentation ιδιαίτερα στα πιο δύσκολα προβλήματα (από 40 κλάσεις και πάνω). Συνεπώς, αν έπρεπε να επιλέξουμε τελικά μοντέλα πιθανώς θα προτιμούσαμε το αρχικό overfitting για τα δυσκολότερα προβλήματα και θα κρατούσαμε το μοντέλο που εκπαιδεύτηκε με data augmentation μόνο στο πρόβλημα των 20 κλάσεων.

### Συμπεράσματα στα "From Scratch" μοντέλα

#### Σχετικά με τη δομή των "From Scratch" μοντέλων

Από τους πειραματισμούς μας με τα "from scratch" μοντέλα φτάσαμε στα εξής συμπεράσματα.

Αρχικά, πρέπει να διευκρινίσουμε πως το μοντέλο στο οποίο καταλήξαμε μπορεί να βελτιωθεί και άλλο ως προς την απόδοσή του αυξάνοντας όμως παράλληλα την πολυπλοκότητά του με ό,τι αυτό συνεπάγεται (παράμετροι ως προς βελτιστοποίηση, χρόνος εκπαίδευσης). Επιλέξαμε να σταματήσουμε την αναζήτηση για ένα "from scratch" μοντέλο σε αυτό που παρουσιάστηκε τελικώς παραπάνω, καθώς αποτελεί ένα δίκτυο όχι τόσο σύνθετο (εκπαιδεύεται εύκολα και γρήγορα), αλλά με ικανοποιητική αποτελεσματικότητα και με επίπεδα δομημένα με τρόπο τέτοιο που καταδεικνύει την κατανόησή μας ως προς τη λειτουργικότητα του κάθε layer.

Συγκεκριμένα, στο τελικό μοντέλο χρησιμοποιήσαμε επίπεδα Dropout με σκοπό την αποφυγή της υπερεκπαίδευσης, επίπεδα BatchNormalization έτσι ώστε να γίνεται επιτυχώς η μεταφορά του σφάλματος κατά την οπισθοδρόμηση και την ανανέωση των βαρών του δικτύου. Επιπλέον, η χρήση MaxPooling2D μας επέτρεψε σε κάθε block-επιπέδων να κρατήσουμε μόνο τα σημαντικότερα χαρακτηριστικά από άποψη έντασης για να τροφοδοτήσουν τα επόμενα επίπεδα, μειώνοντας τη διαστατικότητα της εισόδου σε κάθε block και επιτρέποντας μας να αυξήσιυμε της παραμέτρους και την υπολογιστική πολυπλοκότητα των Conv2D επιπέδων που ακολουθούσαν χωρίς να αυξάνεται δραματικά το χρονικό κόστος εκπαίδευσης.

## Μοντέλα "transfer learning"

### Accuracy Optimization

#### Ορισμός συναρτήσεων

Στο σημείο αυτό έγιναν οι εξής τροποποιήσεις μερικών δωσμένων συναρτήσεων:

* δημιουργία ξεχωριστού dataset για το Xception, ώστε να ικανοποιείται το απαιτούμενο μέγεθος εισόδου 71x71x3.
* τροποποίηση της συνάρτησης *train_model* ώστε να μπορούμε να τη χρησιμοποιήσουμε με ή χωρίς callbacks, για το memory optimization που θα εξεταστεί στη συνέχεια.
"""

# we user prefetch https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch 
# see also AUTOTUNE
# the dataset is now "infinite"

BATCH_SIZE = 128
AUTOTUNE = tf.data.experimental.AUTOTUNE # https://www.tensorflow.org/guide/data_performance

# sort 

def _input_fn(x,y, BATCH_SIZE):
    ds = tf.data.Dataset.from_tensor_slices((x,y))
    ds = ds.shuffle(buffer_size=data_size)
    ds = ds.batch(BATCH_SIZE)
    ds = ds.repeat()
    ds = ds.prefetch(buffer_size=AUTOTUNE)
    return ds

train_ds =_input_fn(x_train,map_label(y_train), BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,map_label(y_val), BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,map_label(y_test), BATCH_SIZE) #PrefetchDataset object

#this is for Xception, with shape (71,71,3)
x_train_xc = tf.image.resize(x_train, (71, 71))
x_val_xc = tf.image.resize(x_val, (71, 71))
x_test_xc = tf.image.resize(x_test, (71, 71))

train_ds_xc =_input_fn(x_train_xc,map_label(y_train), BATCH_SIZE) #PrefetchDataset object
validation_ds_xc =_input_fn(x_val_xc,map_label(y_val), BATCH_SIZE) #PrefetchDataset object
test_ds_xc =_input_fn(x_test_xc,map_label(y_test), BATCH_SIZE) #PrefetchDataset object


# steps_per_epoch and validation_steps for training and validation: https://www.tensorflow.org/guide/keras/train_and_evaluate

def train_model(model_name, model, epochs, steps_per_epoch, validation_steps, verbose, cb = True):
    callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=6, restore_best_weights = True)
    if cb:
        if model_name == 'Xception':
            history = model.fit(train_ds_xc, callbacks=[callback], epochs=epochs, 
                            steps_per_epoch=steps_per_epoch, validation_data=validation_ds_xc, 
                            validation_steps=validation_steps, verbose=verbose)
        else:
            history = model.fit(train_ds, callbacks=[callback], epochs=epochs, 
                            steps_per_epoch=steps_per_epoch, validation_data=validation_ds, 
                            validation_steps=validation_steps, verbose=verbose)
    else:
        if model_name == 'Xception':
            history = model.fit(train_ds_xc, callbacks=[], epochs=epochs, 
                            steps_per_epoch=steps_per_epoch, validation_data=validation_ds_xc, 
                            validation_steps=validation_steps, verbose=verbose)
        else:
            history = model.fit(train_ds, callbacks=[], epochs=epochs, 
                            steps_per_epoch=steps_per_epoch, validation_data=validation_ds, 
                            validation_steps=validation_steps, verbose=verbose)        
    return(history)

# plot diagnostic learning curves
def summarize_diagnostics(history):
	plt.figure(figsize=(8, 8))
	plt.suptitle('Training Curves')
	# plot loss
	plt.subplot(211)
	plt.title('Cross Entropy Loss')
	plt.plot(history.history['loss'], color='blue', label='train')
	plt.plot(history.history['val_loss'], color='orange', label='val')
	plt.legend(loc='upper right')
	# plot accuracy
	plt.subplot(212)
	plt.title('Classification Accuracy')
	plt.plot(history.history['accuracy'], color='blue', label='train')
	plt.plot(history.history['val_accuracy'], color='orange', label='val')
	plt.legend(loc='lower right')
	return plt
 
# print test set evaluation metrics
def model_evaluation(model_name, model, evaluation_steps):
    print('\nTest set evaluation metrics')
    if model_name == 'Xception':
        loss0,accuracy0 = model.evaluate(test_ds_xc, steps = evaluation_steps)
    else:
        loss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)
    print("loss: {:.2f}".format(loss0))
    print("accuracy: {:.2f}".format(accuracy0))

def model_report(model, history, evaluation_steps = 10):
	plt = summarize_diagnostics(history)
	plt.show()
	model_evaluation(model_name, model, evaluation_steps)

IMG_SHAPE = x_train_ds[1].shape

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(4),
  tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.3, seed=5, name=None),
  tf.keras.layers.experimental.preprocessing.PreprocessingLayer(trainable=True, name=None, dtype=None, dynamic=False)
])

"""Μέσω της παρακάτω συνάρτησης δημιουργούμε ένα μοντέλο με μεταφορά γνώσης από ήδη προεκπαιδευμένα μοντέλα. (εντέλει ασχοληθήκαμε με 2 από τα παρακάτω μοντέλα)  
Αρχικά, παίρνουμε τα βάρη από ένα προεκπαιδευμένο μοντέλο. Στη συνέχεια, μέσω fine tuning προσπαθούμε να βελτιστοποιήσουμε την απόδοση του μοντέλου, εξετάζοντας πόσα επίπεδα θα συνεχίσουν την εκπαίδευση και πόσα θα κρατήσουν τα προηγούμενα βάρη.
Τέλος, προσθέτουμε κάποια επιπλέον επίπεδα ανάλογα με τις ανάγκες του κάθε μοντέλου, καθώς και τα κριτήρια βελτιστοποίησής του (πχ ως προς υπερεκπαίδευση).
"""

def transfer_model(model_name, trainable, trainable_layers, 
                   dropout_rate, data_aug, optimizer, loss,
                   epochs, steps_per_epoch, validation_steps, 
                   evaluation_steps, verbose, cb = True):
   
    if model_name == "ResNet152V2":
        model_init = tf.keras.applications.ResNet152V2(input_shape = IMG_SHAPE, include_top = False, weights = 'imagenet')
    elif model_name == "ResNet101":
        model_init = tf.keras.applications.ResNet101(input_shape = IMG_SHAPE, include_top = False, weights = 'imagenet')
    elif model_name == "DenseNet169":
        model_init = tf.keras.applications.DenseNet169(input_shape = IMG_SHAPE, include_top = False, weights = 'imagenet')
    elif model_name == "VGG19":
        model_init = tf.keras.applications.VGG19(input_shape = IMG_SHAPE, include_top = False, weights = 'imagenet')
    elif model_name == "InceptionV3":
        model_init = tf.keras.applications.InceptionV3(include_top = True, weights = 'imagenet')
    elif model_name == "Xception":
        model_init = tf.keras.applications.Xception(input_shape = (71, 71, IMG_SHAPE[2]), include_top = False, weights = 'imagenet')
    else:
        model_init = tf.keras.applications.VGG16(input_shape = IMG_SHAPE, include_top = False, weights = 'imagenet')
  

    MODEL = model_init.layers[0](model_init)
    print("Number of layers in the base model: ", len(MODEL.layers))
    
    # unfreeze conv layers
    MODEL.trainable=True

    if (model_name == "ResNet152V2" or model_name == "ResNet101"):
        for layer in MODEL.layers:
            if isinstance(layer, tf.keras.layers.Conv2D):
                layer.trainable = False
            elif len(layer.trainable_weights) > 0:
                layer.trainable = True
    
    if (trainable==False):
        stop_at = len(MODEL.layers) - trainable_layers 
        for layer in MODEL.layers[:stop_at]:
            layer.trainable =  False
    
    else:
        for layer in MODEL.layers:
            layer.trainable = True
    
    batch_layer = tf.keras.layers.BatchNormalization()
    dropout_layer = tf.keras.layers.Dropout(rate = dropout_rate)
    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
    prediction_layer = tf.keras.layers.Dense(len(our_classes),activation='softmax')
    
    
    if data_aug==True:
        model = tf.keras.Sequential([data_augmentation, MODEL, dropout_layer, global_average_layer, prediction_layer])
    else:
        model = tf.keras.Sequential([MODEL, dropout_layer, global_average_layer, prediction_layer])
    
    model.compile(optimizer= optimizer, loss=loss, metrics=["accuracy"])
    history = train_model(model_name, model, epochs, steps_per_epoch, validation_steps, verbose, cb)

    model_report(model, history, evaluation_steps)
    return model, history

"""Εδώ ορίζουμε τον αριθμό των βημάτων ανά εποχή ανάλογα με το batch_size που χρησιμοποιούμε."""

steps_per_epoch = x_train.shape[0]//BATCH_SIZE
validation_steps = x_val.shape[0]//BATCH_SIZE
evaluation_steps = x_test.shape[0]//BATCH_SIZE

"""Θα εξετάσουμε αναλυτικά τα 2 παρακάτω μοντέλα, σημειώνοντας την επίδραση διαφόρων παραμέτρων στην απόδοσή τους.

* DenseNet169
* Xception

#### **DenseNet169**

To DenseNet-169 αποτελεί ένα δίκτυο στο οποίο κάθε επίπεδο συνδέεται με τα υπόλοιπα επίπεδα σε μία feedforward μορφή. Σε αντίθεση με τα συνηθισμένα CNN L επιπέδων, τα οποία έχουν L συνδέσεις μία με το επόμενο και μία με το προηγούμενο, το DenseNet έχει L(L+1)/2 άμεσες συνδέσεις. Για κάθε επίπεδο οι χάρτες χαρακτηριστικών από όλα τα επίπεδα που έχουν προηγηθεί λαμβάνονται ως είσοδος, ενώ ο χάρτης εξόδου του επιπέδου συμμετέχει ως είσοδος σε όλα τα επίπεδα που το διαδέχονται στο δίκτυο. Τα κυριότερα πλεονεκτήματα αυτού του είδους των Πυκνά Συνδεδεμένων Συνελικτικών Δικτύων είναι ότι αντιμετωπίζουν το πρόβλημα των vanishing gradients, εντείνουν την επαναχρησιμοποίηση και την προσθοδρόμηση των χαρακτηριστικών και  μειώνουν το πλήθος των παρμέτρων του δικτύου.

Η αρχιτεκτονική του δικτύου αυτού περιγράφεται από τις παρακάτω εικόνες:

<a href="https://i.imgur.com/wWHWbQt.png"><img src="https://i.imgur.com/wWHWbQt.png" alt="2" border="0"></a>  

<a href="https://i.imgur.com/oiTdqJL.png"><img src="https://i.imgur.com/oiTdqJL.png" alt="2" border="0"></a>

Για όλες τις δοκιμές χρησιμοποιήσαμε τις ίδιες παραμέτρους, αλλάζοντας κάθε φορά την παράμετρο προς εξέταση, και στο τέλος καταλήξαμε στον καλύτερο συνδυασμό, με βάση την απόδοση του μοντέλου. Όλες οι δοκιμές που ακολουθούν έγιναν για αριθμό κλάσεων ίσο με 20. 

Για την αρχική δοκιμή του μοντέλου χρησιμοποιήσαμε τις παρακάτω τιμές:

* learning rate: 0.00005
* trainable: True
* trainable layers: All
* dropout rate: 0.5
* data augmentation: False
* optimizer: Adam
* loss : Crossentropy
* epochs: 40

Στη συνέχεια, τροποποιήσουμε τις παραπάνω μεταβλητές ως εξής (αναφέρονται μόνο εκείνες που είναι διαφορετικές από τις παρακάτω για κάθε δοκιμή):

1. trainable: False, trainable layers: 0
2. trainable: False, trainable layers: 10
3. trainable: False, trainable layers: 50
4. data augmentation: True
5. learning rate: 0.0005
6. dropout rate: 0.2
7. optimizer: RMSprop

Επίσης, κάναμε και τις παρακάτω δοκιμές, διατηρώντας τις αρχικές παραμέτρους:

8. Θέσαμε το batch size ίσο με 64.
9. Προσθέσαμε ένα batch normalization layer.
10. Θέσαμε το input size ίσο με 71x71x3
"""

models = ["DenseNet169"]
lr = 0.00005
trainable = True
trainable_layers = 0 #afou trainable true einai ashmanto
dropout_rate = 0.5
data_aug = True
optimizer = tf.keras.optimizers.Adam(learning_rate = lr)
loss = tf.keras.losses.sparse_categorical_crossentropy
epochs = 40
verbose=1

start_time = time.time()
for model_name in models:
    model, history = transfer_model(model_name, trainable, trainable_layers, 
                                    dropout_rate, data_aug, optimizer, loss,
                                    epochs, steps_per_epoch, validation_steps, 
                                    evaluation_steps,verbose)
print("--- %s seconds ---" % (time.time() - start_time))

"""Παρακάτω βρίσκονται συγκεντρωτικά όλα τα αποτελέσματα των παραπάνω δοκιμών, μαζί με κάποια συμπεράσματα."""

name = ["standard", "trainable_0", "trainable_10", "trainable_50",
        "data_augmentation","lr=0.0005", "dropout=0.2", "RMSprop optimizer",
        "batch_s=64", "+batch_norm_layer", "resized_71x71"]

test_accuracy = [0.72, 0.52, 0.59, 0.63, 0.7, 0.71, 0.71, 0.7, 0.72, 0.7, 0.4]
test_loss = [1.06, 1.64, 1.35, 1.22, 1.1, 1.41, 1.32, 2.13, 1.41, 1.4, 2.14]
train_accuracy = [0.99, 0.48, 0.68, 0.95, 0.86, 0.99, 0.99, 0.97, 0.99, 0.97, 0.97]
col = ['method', 'test_accuracy', 'test_loss', 'train_accuracy']

df = pd.DataFrame(list(zip(name, test_accuracy, test_loss, train_accuracy)), 
                      columns = col)
df

"""* Σχετικά με το πλήθος των επιπέδων προς εκπαίδευση:  
Παρατηρούμε πως η απόδοση του μοντέλου αυξάνεται όσο κάνουμε "unfreeze" περισσότερα επίπεδα. Ωστόσο, καθώς ξαναεκπαιδεύουμε περισσότερα επίπεδα, κάνουμε πιο σύνθετη την εκπαίδευση, με αποτέλεσμα να γίνεται υπερεκπαίδευση, όπως φαίνεται και από το train accuracy.  
  
  
* Σχετικά με τον optimizer:  
Βέλτιστος είναι o Adam.


* Σχετικά με το data augmentation:  
Εδώ παρατηρούμε πως παρόλο που η απόδοση του συστήματος παρουσιάζει μία μικρή μείωση, η υπερεκπαίδευση ελαττώνεται αισθητά.


* Σχετικά με τις υπόλοιπες παραμέτρους, δεν παρουσιάστηκε αισθητή διαφορά για τις διάφορες τιμές τους.

Συνεπώς, με βάση τα παραπάνω συμπεράσματα, για την τελική μορφή του μοντέλου κρατήσαμε τις αρχικές παραμέτρους, θέτοντας trainable όλα τα επίπεδα, ενώ για να αντιμετωπίσουμε την υπερεκπαίδευση χρησιμοποιήσαμε το data augmentation layer, με ένα μικρό κόστος στην απόδοση. Επίσης, διατηρήσαμε τον αριθμό batch size 128, το αρχικό μέγεθος, ενώ δεν προσθέσαμε το batch normalization layer πριν το dropout.   

Για αυτές λοιπόν τις τιμές, εξετάσαμε την απόδοση του μοντέλου για 80 κλάσεις, με τα αποτελέσματα και τους χρόνους εκπαίδευσης για 20 και 80 κλάσεις να είναι τα εξής:

* 20 κλάσεις:  0.7 test accuracy, 167sec training time
* 80 κλάσεις:  0.55 test accuracy, 598sec training time

Όπως είναι λογικό, ο χρόνος εκπαίδευσης είναι ανάλογος του αριθμού των δεδομένων, άρα και των κλάσεων. Ταυτόχρονα, παρατηρούμε μείωση της απόδοσης του μοντέλου, κάτι το οποίο αναμέναμε αφού υπάρχουν πολύ περισσότερα και διαφορετικά δεδομένα, με πιθανές ομοιότητες, οι οποίες δυσκολεύουν την εκπαίδευση του μοντέλου.

#### **Xception**

Το Xception είναι μία ακραία έκδοση του Inception, η οποία έχει κατασκευάστει από τη Google. Βασίζεται σε μία παραλλαγή της διακριτής συνέλιξης σε βάθος (Depthwise Seperable Convolution). Ενώ η κανονική έκδοση αυτής αποτελείται από μία συνέλιξη σε βάθος που ακολουθείται από μία σημειακή συνέλιξη η παραλλαγή που χρησιμοποιείται στο Xception αντιστρέφει τη σειρά με τη σημειακή συνέλιξη να προηγείται της συνέλιξης σε βάθος. Ένα από τα κύρια πλεονεκτήματα αυτής της μεθόδου είναι ότι παύει πλέον να υπάρχει μη-γραμμικότητα μετά την πρώτη πράξη.   


<a href="https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568"><img src="https://miro.medium.com/max/700/1*J8dborzVBRBupJfvR7YhuA.png" alt="2" border="0"></a>

Η συνολική αρχιτεκτονική του περιγράφεται στην εικόνα που ακολουθεί:    
  
    
  
<a href="https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568"><img src="https://miro.medium.com/max/700/1*hOcAEj9QzqgBXcwUzmEvSg.png" alt="2" border="0"></a>

Θα ακολουθήσουμε την ίδια μεθοδολογία με το δίκτυο DenseNet ως προς την εξέταση των συνδυασμών των υπερπαραμέτρων.
"""

models = ["Xception"]
lr = 0.00005
trainable = True
trainable_layers = 50 #afou trainable true einai ashmanto
dropout_rate = 0.5
data_aug = False
optimizer = tf.keras.optimizers.Adam(learning_rate = lr)
loss = tf.keras.losses.sparse_categorical_crossentropy
epochs = 40
verbose=1

start_time = time.time()
for model_name in models:
    model, history = transfer_model(model_name, trainable, trainable_layers, 
                                    dropout_rate, data_aug, optimizer, loss,
                                    epochs, steps_per_epoch, validation_steps, 
                                    evaluation_steps,verbose)
print("--- %s seconds ---" % (time.time() - start_time))

name_x = ["standard", "data_augmentation", "trainable_0_aug", "trainable_10_aug", 
          "trainable_50_aug", "trainable_0","RMSprop optimizer", "batch_s=64", 
          "+batch_norm_layer"]

test_accuracy_x = [0.83, 0.81, 0.58, 0.74, 0.75, 0.67, 0.82, 0.82, 0.81]
col_x = ['method', 'test_accuracy']

df_x = pd.DataFrame(list(zip(name_x, test_accuracy_x)), 
                      columns = col_x)
df_x

"""Όπως φαίνεται από το παραπάνω dataframe, παρατηρούμε την ίδια συμπεριφορά στις αλλαγές των τιμών των μεταβλητών όπως το βασισμένο στο DenseNet μοντέλο, συνεπώς και εδώ ακολουθήσαμε τον ίδιο συνδυασμό για το βέλτιστο μοντέλο, με τη διαφορά ότι δε χρησιμοποιούμε data augmentation, καθώς σε αυτή την περίπτωση ενώ μειώνεται η απόδοση, η υπερεκπαίδευση παραμένει.

Ο χρόνος εκπαίδευσης καθώς και η απόδοση του μοντέλου για 20 και 80 κλάσεις είναι οι εξής:


* 20 κλάσεις:  0.83 test accuracy, 226sec training time
* 80 κλάσεις:  0.74 test accuracy, 865sec training time

### Hyper-parameter Tuning

Σε αυτό το σημείο, προσθέσαμε κάποια επιπλέον επίπεδα, και κάναμε αναζήτηση του καλύτερου συνδυασμού υπερπαραμέτρων μέσω του ***kerastuner***. Χρησιμοποιήσαμε τη ***RandomSearch*** για να αποφύγουμε την εξαντλητική αναζήτηση.  

Θα το τρέξουμε με αριθμό προσπαθειών ίσο με 4, και για 20 κλάσεις.
"""

from kerastuner import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters

def param_tuning(model_name):
    
    tuner = RandomSearch(init_tuner_model,
                         objective='val_accuracy',
                         max_trials=4,
                         executions_per_trial=1)
    
    tuner.search_space_summary()
    tuner.search(train_ds, epochs = 40, validation_data = validation_ds, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)
    
    hyperparameter_names = ['dropout_1', 'dropout_2' ,'dense_1', 'activation_1', 'learning_rate']
    model = tuner.get_best_models(1)[0]
    best_hyperparameters = tuner.get_best_hyperparameters(1)[0]
    tuner.results_summary()
    for param_tuner in hyperparameter_names:
        print("{} : {}".format(param_tuner, best_hyperparameters.get(param_tuner)))
    test_loss, test_accuracy = model.evaluate(test_ds, steps = evaluation_steps)
    
    return model, best_hyperparameters, test_loss, test_accuracy

def init_tuner_model(par_tuner):
    
    MODEL = tf.keras.applications.DenseNet169(input_shape = IMG_SHAPE, include_top = False, weights = 'imagenet')
    model = tf.keras.Sequential([MODEL])

    model.add(tf.keras.layers.BatchNormalization()),
    model.add(tf.keras.layers.Dropout(rate = par_tuner.Float('dropout_1', min_value = 0.0, max_value = 0.6, default = 0.4, step = 0.1))),
    model.add(tf.keras.layers.GlobalAveragePooling2D()),

    model.add(tf.keras.layers.Dense(units = par_tuner.Int( 'dense_1', min_value = 32, max_value = 256, step = 32, default = 64), activation = par_tuner.Choice( 'activation_1', values=['relu', 'tanh'], default='relu'))),
    model.add(tf.keras.layers.BatchNormalization()),
    model.add(tf.keras.layers.Dropout(rate = par_tuner.Float('dropout_2', min_value=0.0, max_value=0.6, default = 0.2, step=0.1))),

    model.add(tf.keras.layers.Dense(len(our_classes),activation='softmax'))

    model.compile( optimizer = tf.keras.optimizers.Adam( par_tuner.Float('learning_rate', 5e-5, 1e-4, sampling='log')), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

    return model

model_name = "DenseNet169"
DenseNet169_best_tuner, DenseNet169_best_hyperparamters, DenseNet169_test_loss, DenseNet169_test_accuracy  = param_tuning(model_name)

print(DenseNet169_best_hyperparamters.values,'\n')
print("test accuracy: ", DenseNet169_test_accuracy,'\n')
print("test loss: ", DenseNet169_test_loss)

"""Παραπάνω φαίνονται οι τιμές των υπερπαραμέτρων που τελικά επιλέχθηκαν, με την τελική βέλτιστη τιμή του test accuracy να είναι ίση με ***0.72***.  
Ο συνολικός χρόνος που χρειάστηκε για την αναζήτηση και εκπαίδευση του μοντέλου μέσω του *kerastuner* ήταν 15λ:42δ και για τις 4 δοκιμές, με μέσο όρο 3λ:55δ ανά δοκιμή.

Φυσικά, μέσω του *kerastuner* μπορούμε να κάνουμε ακόμα μεγαλύτερη αναζήτηση, να δοκιμάσουμε περισσότερα επίπεδα και επιλογές για τις υπερπαραμέτρους, αλλά αυτό απαιτεί ανάλογα περισσότερο χρόνο.

### Memory Optimization

Στο σημείο αυτό θα χρησιμοποιήσουμε τεχνικές βελτιστοποίησης του δικτύου ως προς τη μνήμη που χρησιμοποιεί και την ταχύτητα, όπως αυτή αποτυπώνεται στη διάρκεια της διαδικασίας εκπαίδευσης.

#### Caching

Για να ελέγξουμε την επιρροή του caching θα εκτελέσουμε το παρακάτω benchmark.

Θα εκπαιδεύσουμε ένα δίκτυο **με** και **χωρίς** caching στο dataset εισόδου και με ακριβώς ίδιες όλες τις άλλες παραμέτρους με σκοπό να δούμε το χρονικό κέρδος από το caching στο σύνολο της εκπαίδευσης.

Θα απενεργοποιήσουμε τα callbacks γιατί δε θέλουμε το Early Stopping να επηρεάσει τα αποτελέσματά μας για το λόγω αυτό θα κατασκευάσουμε μία νέα συνάρτηση για το input `με caching` και μία νέα συνάρτηση για training `χωρίς callbacks`

**Σημειώνουμε ότι για αυτό και μόνο το μέρος της εργασίας δεν επικεντρωνόμαστε σε αποτελέσματα που έχουν να κάνουν με το accuracy ούτε χρησιμοποιούμε τις βέλτιστες για αυτά παραμέτρους αλλά επικεντρωνόμαστε ΕΞ ΟΛΟΚΛΗΡΟΥ σε χρόνους εκτέλεσης**
"""

models = ["DenseNet169"]

lr = 0.0001
trainable = False
trainable_layers = 0 #afou trainable true einai ashmanto
dropout_rate = 0.5
data_aug = False
optimizer = tf.keras.optimizers.Adam(learning_rate = lr)
loss = tf.keras.losses.sparse_categorical_crossentropy
epochs = 40
verbose=1

for model_name in models:
    model_start_time = time.time()
    model, history = transfer_model(model_name, trainable, trainable_layers, 
                                    dropout_rate, data_aug, optimizer, loss,
                                    epochs, steps_per_epoch, validation_steps, 
                                    evaluation_steps,verbose, 0, False)
    model_duration = time.time() - model_start_time

print("The total training time for model DenseNet169 with prefetching was: ", model_duration)

models = ["Xception"]

lr = 0.0001
trainable = False
trainable_layers = 0 #afou trainable true einai ashmanto
dropout_rate = 0.5
data_aug = False
optimizer = tf.keras.optimizers.Adam(learning_rate = lr)
loss = tf.keras.losses.sparse_categorical_crossentropy
epochs = 40
verbose=1

for model_name in models:
    model_start_time = time.time()
    model, history = transfer_model(model_name, trainable, trainable_layers, 
                                    dropout_rate, data_aug, optimizer, loss,
                                    epochs, steps_per_epoch, validation_steps, 
                                    evaluation_steps,verbose, 0, False)
    model_duration = time.time() - model_start_time

print("The total training time for model Xception with prefetching was: ", model_duration)

"""Παρακάτω επανυλοποιούμε την συνάρτηση εισόδου, ώστε αυτή να περιλαμβάνει caching και επαναλαμβάνουμε το πείραμα με σκοπό να συγκρίνουμε τους συνολικούς χρόνους. Επιλέξαμε να συγκρίνουμε χρόνους για τα μοντέλα που κάναμε και optimization στα υπόλοιπα μέρη της εργασίας."""

BATCH_SIZE = 128
AUTOTUNE = tf.data.experimental.AUTOTUNE # https://www.tensorflow.org/guide/data_performance

# sort 

def _input_fn(x,y, BATCH_SIZE):
    ds = tf.data.Dataset.from_tensor_slices((x,y))
    ds = ds.shuffle(buffer_size=data_size)
    ds = ds.batch(BATCH_SIZE)
    ds = ds.repeat()
    #ds = ds.prefetch(buffer_size=AUTOTUNE)
    ds = ds.cache()
    return ds

train_ds =_input_fn(x_train,map_label(y_train), BATCH_SIZE) #PrefetchDataset object
validation_ds =_input_fn(x_val,map_label(y_val), BATCH_SIZE) #PrefetchDataset object
test_ds =_input_fn(x_test,map_label(y_test), BATCH_SIZE) #PrefetchDataset object

x_train_xc = tf.image.resize(x_train, (71, 71))
x_val_xc = tf.image.resize(x_val, (71, 71))
x_test_xc = tf.image.resize(x_test, (71, 71))

train_ds_xc =_input_fn(x_train_xc,map_label(y_train), BATCH_SIZE) #PrefetchDataset object
validation_ds_xc =_input_fn(x_val_xc,map_label(y_val), BATCH_SIZE) #PrefetchDataset object
test_ds_xc =_input_fn(x_test_xc,map_label(y_test), BATCH_SIZE) #PrefetchDataset object

models = ["DenseNet169"]

lr = 0.0001
trainable = False
trainable_layers = 0 #afou trainable true einai ashmanto
dropout_rate = 0.5
data_aug = False
optimizer = tf.keras.optimizers.Adam(learning_rate = lr)
loss = tf.keras.losses.sparse_categorical_crossentropy
epochs = 40
verbose=1

for model_name in models:
    model_start_time = time.time()
    model, history = transfer_model(model_name, trainable, trainable_layers, 
                                    dropout_rate, data_aug, optimizer, loss,
                                    epochs, steps_per_epoch, validation_steps, 
                                    evaluation_steps,verbose, 0, False)
    model_duration = time.time() - model_start_time

print("The total training time for model DenseNet169 with caching was: ", model_duration)

models = ["Xception"]

lr = 0.0001
trainable = False
trainable_layers = 0 #afou trainable true einai ashmanto
dropout_rate = 0.5
data_aug = False
optimizer = tf.keras.optimizers.Adam(learning_rate = lr)
loss = tf.keras.losses.sparse_categorical_crossentropy
epochs = 40
verbose=1

for model_name in models:
    model_start_time = time.time()
    model, history = transfer_model(model_name, trainable, trainable_layers, 
                                    dropout_rate, data_aug, optimizer, loss,
                                    epochs, steps_per_epoch, validation_steps, 
                                    evaluation_steps,verbose, 0, False)
    model_duration = time.time() - model_start_time

"""Για το μοντέλο Xception δεν καταφέραμε καν να φτάσουμε σε αποτέλεσμα με caching καθώς το Kaggle συνεχώς κατέρρεε από υπέρβαση μνήμης.

#### Γενικά συμπεράσματα ως προς Memory Optimization

Σε γενικές γραμμές παρατηρούμε πως δεν υπάρχει βελτίωση, ενώ την ίδια στιγμή έχουμε να αντιμετωπίσουμε συνεχώς και την κατάρρευση του πυρήνα στο Kaggle λόγω υπέρβασης χρήσης μνήμης. Έτσι λοιπόν, το caching είναι μία τακτική την οποία θα αποφεύγαμε σε αυτό το Notebook σε αντίθεση με την Prefetch που χρησιμοποιούμε από την αρχή.
"""